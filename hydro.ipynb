{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Подготовка списка датафреймов по соответствующим постам\n",
    "И списка ID выбранных постов\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "path = '/mnt/SSD/education/aspirantura/CAMELS_ru/files/hydrology/2020-09-22_15-39'\n",
    "all_files = glob.glob(path + '/*_Q_Day.Cmd.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Store Lat, Lon values as tuple with same\n",
    "position as index of station\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "GRDC_Stations = pd.read_excel(path + '/GRDC_Stations.xlsx')\n",
    "Valid_lat_lon = list()\n",
    "for j in range(len(Valid_WS)):\n",
    "    for i in range(len(temporary)):    \n",
    "        if Valid_WS.ID.loc[j] == temporary.grdc_no.loc[i]:\n",
    "            Valid_lat_lon.append((temporary.lat.loc[i], temporary.long.loc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "количество постов с убранными пустышками 211, всего постов с суточными данными 1491\n"
     ]
    }
   ],
   "source": [
    "list_of_gauges = list()\n",
    "list_of_station_IDS = [i[74:81] for i in all_files] # лист всех ID из представленных постов\n",
    "valid_IDS = list()\n",
    "for i, gauge in enumerate(all_files):\n",
    "    \n",
    "    \"\"\"\n",
    "    skip instance if it's empty\n",
    "    \n",
    "    \"\"\"\n",
    "    test = pd.read_csv(gauge, skiprows = 36, delimiter = ';', skipinitialspace = True)\n",
    "    \n",
    "    if test.empty:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \"\"\"\n",
    "        read data; \n",
    "        change default column names;\n",
    "        replace -999 with NaN;\n",
    "        drop unnecessary column;\n",
    "        add column with baseflow calculated with BFI function\n",
    "        add it to list of valid time serieses \n",
    "        \n",
    "        \"\"\"\n",
    "        test.Value = test.Value.replace(-999, np.NaN)\n",
    "        test['YYYY-MM-DD'] = pd.to_datetime(test['YYYY-MM-DD'])\n",
    "        test = test.drop(columns = ['hh:mm'])\n",
    "        test.rename(columns = {'YYYY-MM-DD': 'Date', 'Value': 'Q'}, inplace = True)\n",
    "        test['Qbase'] = np.NaN\n",
    "        list_of_gauges.append(test)\n",
    "        valid_IDS.append(int(list_of_station_IDS[i]))\n",
    "        \n",
    "print('количество постов с убранными пустышками {}, всего постов с суточными данными {}'.format(len(list_of_gauges), len(all_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Чтение geojson\n",
    "Создание списка атрибутов всех водосборов с площадью\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import geojson as gjs\n",
    "with open(path + '/stationbasins.geojson') as f:\n",
    "    WS_info = gjs.load(f)\n",
    "\n",
    "WS_info = [i['attributes'] for i in WS_info['features']]\n",
    "\n",
    "Valid_WS_Info = [WS_info[i] for i in range(len(WS_info)) if WS_info[i]['grdc_no'] in valid_IDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "количество постов с суточными данными и площадью 186, всего постов с суточными данными 211\n"
     ]
    }
   ],
   "source": [
    "Valid_WS = pd.DataFrame(columns = {'ID', 'River_Name', 'WS_area'}) # dataframe for WS data\n",
    "Valid_WS.ID = valid_IDS # assign only valuable WS with daily discharge available\n",
    "\n",
    "temp_list_for_AREA = [[] for _ in range(len(Valid_WS.ID))] # empty list to store river name\n",
    "# list of lists is used as a solution to solve .loc copy problem from df\n",
    "temp_list_for_RIVER_NAME = [[] for _ in range(len(Valid_WS.ID))]\n",
    "\n",
    "for j in range(len(Valid_WS.ID)):\n",
    "    for i in range(len(Valid_WS_Info)):\n",
    "        fnd = True\n",
    "        while fnd:\n",
    "            \n",
    "            fnd = False\n",
    "            \n",
    "            if Valid_WS.ID.loc[j] == Valid_WS_Info[i]['grdc_no']:\n",
    "                temp_list_for_AREA[j].append(Valid_WS_Info[i]['area_hys'])\n",
    "                temp_list_for_RIVER_NAME[j].append(Valid_WS_Info[i]['river'] + ' - ' + Valid_WS_Info[i]['station'])\n",
    "                \n",
    "                fnd = True\n",
    "                break\n",
    "                \n",
    "temp_list_for_AREA = [np.NaN if len(i) == 0 else i[0] for i in temp_list_for_AREA]\n",
    "temp_list_for_RIVER_NAME = [np.NaN if len(i) == 0 else i[0] for i in temp_list_for_RIVER_NAME]\n",
    "\n",
    "\"\"\"\n",
    "Датасет сформирован таким образом, что здесь используются только те посты, \n",
    "по которым есть данные суточного стока\n",
    "Расположены они в том же порядке, что и данные по расходам в\n",
    "переменной\n",
    "\n",
    "Далее, обращаясь к ID, получаем данные по площади водосбора,\n",
    "которые используются дальше, при получении миллиметров слоя\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Valid_WS.River_Name = temp_list_for_RIVER_NAME\n",
    "Valid_WS.WS_area = temp_list_for_AREA\n",
    "Valid_WS = Valid_WS[[Valid_WS.columns.tolist()[1], Valid_WS.columns.tolist()[2], Valid_WS.columns.tolist()[0]]]\n",
    "\n",
    "print('количество постов с суточными данными и площадью {}, всего постов с суточными данными {}'.format(len(Valid_WS.WS_area.dropna()), len(Valid_WS.ID.dropna())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Дальнейший расчёт будет вестись только для тех постов,\n",
    "у которых есть данные о площади\n",
    "\n",
    "\"\"\"\n",
    "Valid_WS = Valid_WS.dropna().reset_index(drop = True)\n",
    "Valid_gauges = [gauge for i, gauge in enumerate(list_of_gauges) if valid_IDS[i] in Valid_WS.ID.to_numpy()]\n",
    "\n",
    "import copy\n",
    "\n",
    "Valid_gauges_Q_mm = copy.deepcopy(Valid_gauges)\n",
    "\n",
    "\"\"\"\n",
    "calculate layer of discharge\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i, gauge in enumerate(Valid_gauges_Q_mm):\n",
    "    gauge.Q = 86400 * gauge.Q * 10**9 / (Valid_WS.WS_area.loc[i] * 10**12)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_year(list_of_something):\n",
    "    \n",
    "    \"\"\"\n",
    "    Данная функция разбивает лист датафреймов по годам\n",
    "    и выкидывает года, где NaN больше, чем 2 месяца\n",
    "    \n",
    "    Колонка Date в датафрейме внутри листа - даты\n",
    "    и она обязательна. \n",
    "    Столбцы могут называться в соответствии с пожеланиями\n",
    "    \n",
    "    \"\"\"\n",
    "    splitted_list = list()\n",
    "\n",
    "    for i, gauge in enumerate(Valid_gauges):\n",
    "        \"\"\"\n",
    "        From list of lists we are calling another list \n",
    "        that consist list of dataframes\n",
    "\n",
    "        \"\"\"\n",
    "        year_split = list()\n",
    "        unique_years = gauge.Date.dt.year.unique()\n",
    "\n",
    "        for year in unique_years:\n",
    "            year_split.append(gauge[gauge.Date.dt.year == year].reset_index(drop = True))\n",
    "\n",
    "        for i in range(len(year_split)-1, -1, -1):\n",
    "            if sum(year_split[i].Q.isna()) > len(year_split[i].Q)/6:\n",
    "                del year_split[i]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        splitted_list.append(year_split)\n",
    "    \n",
    "    return splitted_list\n",
    "\n",
    "every_gauge_split_by_year = split_by_year(Valid_gauges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "После фильтрации оказалось, что некоторые годы на постах имеют в себе Большое количество пропусков\n",
    "\n",
    "\"\"\"\n",
    "Valid_gauges_Q_mean_yearly = [np.mean([yearly_sum.Q.sum() for yearly_sum in gauge]) if len([yearly_sum.Q.sum() for yearly_sum in gauge]) > 2 else np.NaN for gauge in every_gauge_split_by_year]\n",
    "\n",
    "print('количество постов с рассчитанным средним {}, всего постов с суточными данными {}'.format(\n",
    "    len(np.array(Valid_gauges_Q_mean_yearly)[~np.isnan(Valid_gauges_Q_mean_yearly)]),\n",
    "    len(Valid_gauges_Q_mean_yearly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slope_fdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanquantile(test.Q.to_numpy(), q = 0.33), np.nanquantile(test.Q.to_numpy(), q = 0.66), np.nanquantile(test.Q.to_numpy(), q = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    (math.log(np.nanquantile(test.Q.to_numpy(), q = 0.33)) - math.log(np.nanquantile(test.Q.to_numpy(), q = 0.66)))/(0.66 - 0.33)\n",
    "except ValueError:\n",
    "    print('33 перцентиль = {}, 66 перцентиль = {}'.format(np.nanquantile(test.Q.to_numpy(), q = 0.33), np.nanquantile(test.Q.to_numpy(), q = 0.66)))\n",
    "finally:\n",
    "    print(\n",
    "        (math.log(np.nanquantile(test.Q.to_numpy(), q = 0.33)+0.0009) - math.log(np.nanquantile(test.Q.to_numpy(), q = 0.66)+0.001))/(0.66 - 0.33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(test.Q)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseflow_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clump_array(a):\n",
    "\n",
    "    \"\"\"\n",
    "    Разбить период наблюдений на куски, в которых нет NaN\n",
    "    \"\"\"\n",
    "\n",
    "    return [a[s] for s in np.ma.clump_unmasked(np.ma.masked_invalid(a))]\n",
    "\n",
    "\"Создаём отдельные вектора в соответствии с разбиением на куски без NaN\"\n",
    "\n",
    "###################################################################\n",
    "def FirstPass(Q, alpha):\n",
    "\n",
    "    q_f_1 = [[[] for _ in range(len(i))] for i in Q]\n",
    "    q_b_1 = [[[] for _ in range(len(i))] for i in Q]\n",
    "\n",
    "    for i in range(len(Q)):           \n",
    "        \"\"\"\n",
    "        Задаём первые величины векторе \"быстрого\" стока\n",
    "\n",
    "        \"\"\"\n",
    "        q_f_1[i][0] = Q[i][0]\n",
    "\n",
    "        for j in range(len(Q[i])-1):\n",
    "            \"\"\"\n",
    "            Для каждого куска считаем быстрый сток\n",
    "\n",
    "            \"\"\"\n",
    "            q_f_1[i][j+1] = alpha * q_f_1[i][j] + 0.5 * (1 + alpha) * (Q[i][j+1] - Q[i][j])\n",
    "\n",
    "        for j in range(len(Q[i])):\n",
    "            if q_f_1[i][j] < 0:\n",
    "                q_b_1[i][j] = Q[i][j]\n",
    "            else:\n",
    "                q_b_1[i][j] = Q[i][j] - q_f_1[i][j]\n",
    "\n",
    "    Q_forward_1 = [q_f_1, q_b_1]\n",
    "\n",
    "    return Q_forward_1\n",
    "\n",
    "###################################################################\n",
    "\n",
    "def BackwardPass(Q_forward_1, alpha):\n",
    "\n",
    "    \"\"\"\n",
    "    Здесь Q - n-мерный лист в зависимости от числа разбиений\n",
    "    \"\"\"\n",
    "\n",
    "    Qq = Q_forward_1[0]\n",
    "    Qb = Q_forward_1[1]\n",
    "\n",
    "    q_f_2 = [[[] for _ in range(len(i))] for i in Qq]\n",
    "    q_b_2 = [[[] for _ in range(len(i))] for i in Qb]\n",
    "\n",
    "    for i in range(len(Qq)):\n",
    "        \"последняя величина предыдущего шага - первая в обратном\"\n",
    "        q_f_2[i][-1] = Qb[i][-1]\n",
    "\n",
    "        for j in range(len(Qq[i])-2, -1, -1):\n",
    "            q_f_2[i][j] = alpha * q_f_2[i][j+1] + 0.5 * (1 + alpha) * (Qb[i][j] - Qb[i][j+1])\n",
    "\n",
    "        for j in reversed(range(len(Qq[i]))):\n",
    "            if q_f_2[i][j] < 0:\n",
    "                q_b_2[i][j] = Qb[i][j]\n",
    "            else:\n",
    "                q_b_2[i][j] = Qb[i][j] - q_f_2[i][j]\n",
    "    Q_backward = [q_f_2, q_b_2]\n",
    "\n",
    "    return Q_backward\n",
    "\n",
    "###################################################################\n",
    "\n",
    "def ForwardPass(Q_backward, alpha):\n",
    "\n",
    "    Qq = Q_backward[0]\n",
    "    Qb = Q_backward[1]\n",
    "\n",
    "    q_f_3 = [[[] for _ in range(len(i))] for i in Qq]\n",
    "    q_b_3 = [[[] for _ in range(len(i))] for i in Qb]\n",
    "\n",
    "    for i in range(len(Qq)):\n",
    "\n",
    "        \"Теперь первая величина предыдущего шага - первая и здесь\"\n",
    "\n",
    "        q_f_3[i][0] = Qb[i][0]\n",
    "\n",
    "        for j in range(len(Qb[i])-1):\n",
    "\n",
    "            q_f_3[i][j+1] = alpha * q_f_3[i][j] + 0.5 * (1 + alpha) * (Qb[i][j+1] - Qb[i][j])\n",
    "\n",
    "        for j in range(len(Qb[i])):\n",
    "            if q_f_3[i][j] < 0:\n",
    "                q_b_3[i][j] = Qb[i][j]\n",
    "            else:\n",
    "                q_b_3[i][j] = Qb[i][j] - q_f_3[i][j]\n",
    "\n",
    "    Q_forward = [q_f_3, q_b_3]\n",
    "\n",
    "    return Q_forward\n",
    "\n",
    "###################################################################\n",
    "\n",
    "def BFI_calc(Q, alpha, passes, reflect):\n",
    "    \"\"\"\n",
    "    we reflect the first reflect values and the last reflect values.  \n",
    "    this is to get rid of 'warm up' problems © Anthony Ladson\n",
    "    \"\"\" \n",
    "    \n",
    "    Qin = Q\n",
    "    \n",
    "    \"отложим отраженные величины с конца и начала нашего ряда\"\n",
    "    Q_reflect = [[[] for _ in range(len(i) + 2 * reflect)] for i in Q]\n",
    "\n",
    "    for i in range(len(Q_reflect)):\n",
    "\n",
    "        Q_reflect[i][:reflect] = Q[i][(reflect):0:-1] #добавляем \n",
    "\n",
    "        Q_reflect[i][(reflect):(reflect + len(Q[i]))] = Q[i]\n",
    "\n",
    "        Q_reflect[i][(reflect + len(Q[i])):(len(Q[i]) + 2 + 2 * reflect)] = Q[i][len(Q)-2:len(Q) - reflect - 2:-1]\n",
    "    \n",
    "    Q1 = FirstPass(Q_reflect, alpha)\n",
    "\n",
    "    \"how many backwards/forward passes to we need © Anthony Ladson\"\n",
    "\n",
    "    n_pass = round(0.5 * (passes -1))\n",
    "\n",
    "    BackwardPass(Q1, alpha)\n",
    "\n",
    "    for i in range(n_pass):\n",
    "        Q1 = ForwardPass(BackwardPass(Q1, alpha), alpha)\n",
    "\n",
    "    ################# end of passes  ##############################\n",
    "    \n",
    "    Qbase = [i[reflect:(len(i)-reflect)] for i in Q1[1]]\n",
    "    Qbase = [[0 if j < 0 else j for j in i] for i in Qbase]\n",
    "    \n",
    "    # расчёт взвешенного среднего для разбитого на куски ряда расходов\n",
    "    \n",
    "    len_Qbase = len([item for sublist in Qbase for item in sublist]) # длина всех расходов, без учёта пропусков\n",
    "    weights_for_average = [len(i)/len_Qbase for i in Qbase] # вес для среднего по каждому элементу\n",
    "    weighted_bfi = sum([weights_for_average[i] * np.mean(Qbase[i]/np.mean(Qin[i])) for i in range(len(Qbase))]) #взвешенное среднее по выпуклой комбинации\n",
    "\n",
    "    return weighted_bfi, Qbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### проверка на базовом датасете Anthony Ladson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "url = \"https://raw.github.com/TonyLadson/data/master/data/BassRiver@Loch.csv\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    download = s.get(url)\n",
    "\n",
    "    decoded_content = download.content.decode('utf-8')\n",
    "\n",
    "    cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "    my_list = list(cr)\n",
    "\n",
    "my_list = [int(i[0]) for i in my_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "alpha_coefficients = [random.uniform(0.9, 0.98) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_for_bfi[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "на расчёт итерации 0 ушло -116.56409573554993\n",
      "на расчёт итерации 1 ушло -185.62007665634155\n",
      "на расчёт итерации 2 ушло -38.93612599372864\n",
      "на расчёт итерации 3 ушло -167.1171293258667\n",
      "на расчёт итерации 4 ушло -36.936737060546875\n",
      "на расчёт итерации 5 ушло -32.5407919883728\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-346bc9ae3bd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malpha_coefficients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtemp_for_bfi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBFI_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclump_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreflect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtemp_for_Qbase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBFI_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclump_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreflect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'на расчёт итерации {} ушло {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f02cef2a2509>\u001b[0m in \u001b[0;36mBFI_calc\u001b[0;34m(Q, alpha, passes, reflect)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mQ_reflect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreflect\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreflect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreflect\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mQ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFirstPass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_reflect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;34m\"how many backwards/forward passes to we need © Anthony Ladson\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f02cef2a2509>\u001b[0m in \u001b[0;36mFirstPass\u001b[0;34m(Q, alpha)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mq_f_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mq_b_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "temp_for_bfi = [[[] for _ in range(len(gauge))] for gauge in every_gauge_split_by_year]\n",
    "temp_for_Qbase = [[[] for _ in range(len(gauge))] for gauge in every_gauge_split_by_year]\n",
    "\n",
    "passes = 3\n",
    "reflect = 30\n",
    "import time\n",
    "\n",
    "for j, gauge in enumerate(every_gauge_split_by_year):\n",
    "    start_time = time.time()\n",
    "    for k, year in enumerate(gauge):\n",
    "        for alpha in alpha_coefficients:\n",
    "            temp_for_bfi[j][k].append(BFI_calc(clump_array(year.Q.to_numpy()), alpha, passes = 3, reflect = 30)[0])\n",
    "            temp_for_Qbase[j][k].append(BFI_calc(clump_array(year.Q.to_numpy()), alpha, passes = 3, reflect = 30)[1])\n",
    "    stop = time.time()\n",
    "    print('на расчёт итерации {} ушло {}'.format(j, stop-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clump_array(every_gauge_split_by_year[j][k].Q)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q = clump_array(every_gauge_split_by_year[0][0].Q.to_numpy())\n",
    "\n",
    "\n",
    "alpha = 0.98\n",
    "\n",
    "bfi = BFI_calc(Q, 0.98, 3, 30)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4943865494331915"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BFI_calc(clump_array(year), alpha_coefficients[i], passes = 3, reflect = 30)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Восстановления ряда baseflow с учётом ранее \"выбитых\" пропусков\n",
    "\"\"\"\n",
    "from itertools import groupby\n",
    "Qbase_full = [list(group) for key, group in groupby(test.Q.to_numpy(), key=np.isnan)] # разбитие на группы по признаку. Получается n листов. NaN и !NaN разбиты по листам\n",
    "mask_to_refill = [~np.isnan(i).any() for i in Qbase_full] # индексируем их как одинокие булевые величины\n",
    "\n",
    "position_index = -1\n",
    "for j, mask in enumerate(mask_to_refill):\n",
    "    \"\"\"\n",
    "    Так как первичный разрезанный лист из функции clumped_array имеет в себе количество листов, такое же как и True, то восстанавливаем в соответствии\n",
    "    \n",
    "    Аргумент position_index подбирается в соответствии с присутсвием с True и адресуется к нужному куску в Qbase\n",
    "    \n",
    "    Всё перезаписывается в лист, где в перезаписываемых значениях эквивалентные по длине значения стока\n",
    "    \"\"\"\n",
    "    if mask:\n",
    "        position_index += 1\n",
    "        Qbase_full[j] = Qbase[position_index]\n",
    "    else:\n",
    "        position_index = position_index\n",
    "        Qbase_full[j] = Qbase_full[j]\n",
    "            \n",
    "Qbase_full = [item for sublist in Qbase_full for item in sublist] # лист листов в лист с учётом NaN\n",
    "\n",
    "test.Qbase = Qbase_full # Добавляем его в исходный датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_year = [test[test['Date'].dt.year == y].reset_index(drop = True) for y in test['Date'].dt.year.unique()]\n",
    "for i in split_by_year:\n",
    "    i = i.set_index('Date')\n",
    "    ax = i.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hfd_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_year = [test[test['Date'].dt.year == y].reset_index(drop = True) for y in test['Date'].dt.year.unique()]\n",
    "\n",
    "hfd = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(split_by_year)):\n",
    "    half_discharge = 0\n",
    "    for j, discharge in enumerate(split_by_year[i].Q):    \n",
    "        half_discharge += discharge\n",
    "        if half_discharge > split_by_year[i].Q.sum()/2:\n",
    "            break\n",
    "    hfd.append(split_by_year[i].Date[j])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('geo': conda)",
   "language": "python",
   "name": "python38564bitgeoconda80ab28450b61475ea094e6781ce7b782"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
