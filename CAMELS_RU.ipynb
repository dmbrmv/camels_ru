{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "civil-combining",
   "metadata": {},
   "source": [
    "#### READ ME\n",
    "##### Чтение геометрии: GeoJSON - ID, gauge_name, геометрия\n",
    "##### Чтение гидрологии: CSV - имя файла (ID поста), Дата, Расход\n",
    "##### По геометрии в WGS 84 считается площадь для расчёта слоя стока"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accessory-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fiona # требуется LINUX\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-overall",
   "metadata": {},
   "source": [
    "#### Указание обязательных путей хранения файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vocational-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrology_data_path = '/mnt/c/education/HSI/aspirantura/CAMELS_ru/files/Uni_Input/hydrological_series/' # путь до папки с файлами расходов\n",
    "geometry_data_path = '/mnt/c/education/HSI/aspirantura/CAMELS_ru/files/Uni_Input/geometry/' # путь до файла GeoJSON. В файлах \n",
    "path_to_results = '/mnt/c/education/HSI/aspirantura/CAMELS_ru/code/camels_ru/results/'\n",
    "\n",
    "#!TODO заменить на PostgresSQL\n",
    "HydroATLAS_data_path = '/mnt/c/education/HSI/aspirantura/CAMELS_ru/files/HydroAtlas/Data/' # путь до .gdb файла HydroATLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "occupied-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_files_for_CAMELS(file_path_hydrology, file_path_geometry):# geometry = False, hydrology = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    function which reads file with given format:\n",
    "    .geojson for geometry\n",
    "    \n",
    "    Return GeoDataFrame with 4 columns\n",
    "    \n",
    "    'gauge_name', 'ID', 'geometry' and 'area'\n",
    "    \n",
    "    Input file should have same column names !\n",
    "    \n",
    "    .csv for hydrology data\n",
    "    \n",
    "    In this case it's return list of dataframes: each dataframe for gauge\n",
    "    DataFrame consist 3 columns\n",
    "    \n",
    "    'date', 'layer', 'discharge', 'baseflow'\n",
    "    \"\"\"\n",
    "           \n",
    "    def polygon_area(lats, lons, radius = 6378137):\n",
    "        \"\"\"\n",
    "        Computes area of spherical polygon, assuming spherical Earth. \n",
    "        Returns result in ratio of the sphere's area if the radius is specified.\n",
    "        Otherwise, in the units of provided radius.\n",
    "        lats and lons are in degrees.\n",
    "        \"\"\"\n",
    "        from numpy import arctan2, cos, sin, sqrt, pi, power, append, diff, deg2rad\n",
    "        lats = np.deg2rad(lats)\n",
    "        lons = np.deg2rad(lons)\n",
    "\n",
    "        # Line integral based on Green's Theorem, assumes spherical Earth\n",
    "\n",
    "        #close polygon\n",
    "        if lats[0]!=lats[-1]:\n",
    "            lats = append(lats, lats[0])\n",
    "            lons = append(lons, lons[0])\n",
    "\n",
    "        #colatitudes relative to (0,0)\n",
    "        a = sin(lats/2)**2 + cos(lats)* sin(lons/2)**2\n",
    "        colat = 2*arctan2( sqrt(a), sqrt(1-a) )\n",
    "\n",
    "        #azimuths relative to (0,0)\n",
    "        az = arctan2(cos(lats) * sin(lons), sin(lats)) % (2*pi)\n",
    "\n",
    "        # Calculate diffs\n",
    "        # daz = diff(az) % (2*pi)\n",
    "        daz = diff(az)\n",
    "        daz = (daz + pi) % (2 * pi) - pi\n",
    "\n",
    "        deltas=diff(colat)/2\n",
    "        colat=colat[0:-1]+deltas\n",
    "\n",
    "        # Perform integral\n",
    "        integrands = (1-cos(colat)) * daz\n",
    "\n",
    "        # Integrate \n",
    "        area = abs(sum(integrands))/(4*pi)\n",
    "\n",
    "        area = min(area,1-area)\n",
    "        if radius is not None: #return in units of radius\n",
    "            return area * 4 * pi* radius**2 / 10**6\n",
    "        else: #return in ratio of sphere total area\n",
    "            return area / 10**6\n",
    "\n",
    "    from shapely.geometry import Polygon, MultiPolygon\n",
    "        \n",
    "    selected_files = glob.glob(file_path_geometry + '*.geojson') # Select every file from folder. There's should be only one\n",
    "# написать потом обработку отдельных GeoJSON для каждого водосбора        \n",
    "    file = open(selected_files[0])\n",
    "    df = gpd.read_file(file)\n",
    "    file.close()\n",
    "\n",
    "    for geometry_row in range(len(df.geometry)): \n",
    "        if type(df.geometry[geometry_row]) == MultiPolygon: # It's simplify next manipulations with geometry Data\n",
    "\n",
    "            from shapely.ops import cascaded_union\n",
    "\n",
    "            df.geometry[geometry_row] = cascaded_union(df.geometry[geometry_row])\n",
    "\n",
    "        elif type(df.geometry[geometry_row]) == Polygon:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            print('There is not supported type of geometry: {}. Should be shapely.Polygon or shapely.MultiPolygon'.format(type(geometry_row)))\n",
    "\n",
    "    df['Area'] = [polygon_area(lats = df.geometry[row].exterior.coords.xy[1], lons = df.geometry[row].exterior.coords.xy[0]) for row in range(len(df.geometry))]\n",
    "    \n",
    "    \n",
    "################################################################################################################################################################\n",
    "\n",
    "\n",
    "    selected_files = glob.glob(file_path_hydrology + '*.csv') # For each gauge there's own file with ID of watershed\n",
    "    file = [pd.read_csv(file) for file in selected_files]\n",
    "    ID = [int(ID[-9:-4]) if int(ID[-9:-4]) != 99999 else np.NaN for ID in selected_files] # select only identified ID's\n",
    "\n",
    "    for gauge_dataframe in file:\n",
    "        gauge_dataframe['date'] = pd.to_datetime(gauge_dataframe['date']) # assign datetime format for date column\n",
    "    \n",
    "    \n",
    "    for i, gauge_dataframe in enumerate(file):\n",
    "        for j in range(len(df)):\n",
    "            fnd = True\n",
    "            while fnd:\n",
    "\n",
    "                fnd = False\n",
    "\n",
    "                if df.ID[j] == ID[i]:\n",
    "                    layer = 86400 * gauge_dataframe.discharge * 10**9 / (df.Area[j] * 10**12)\n",
    "                    gauge_dataframe.insert(loc = 1, column = 'layer', value = layer) \n",
    "\n",
    "                    fnd = True\n",
    "                    break\n",
    "\n",
    "    for gauge_dataframe in file:\n",
    "        gauge_dataframe['baseflow'] = np.NaN # add column for baseflow\n",
    "\n",
    "    file = [gauge for i, gauge in enumerate(file) if np.isnan(ID[i]) == False] # return only files with given ID's\n",
    "    ID = [i for i in ID if np.isnan(i) == False]\n",
    "\n",
    "    return file, df, ID # returning list of DataFrames and List of Common ID's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "northern-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrology_data, geometry_data, ID = open_files_for_CAMELS(hydrology_data_path, geometry_data_path)\n",
    "# geometry_data = open_files_for_CAMELS(, geometry = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "closing-mortality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>layer</th>\n",
       "      <th>discharge</th>\n",
       "      <th>baseflow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0.116424</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-02</td>\n",
       "      <td>0.114023</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-03</td>\n",
       "      <td>0.114023</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-04</td>\n",
       "      <td>0.112823</td>\n",
       "      <td>0.94</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-05</td>\n",
       "      <td>0.106822</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17162</th>\n",
       "      <td>2016-12-27</td>\n",
       "      <td>1.440296</td>\n",
       "      <td>12.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17163</th>\n",
       "      <td>2016-12-28</td>\n",
       "      <td>1.368282</td>\n",
       "      <td>11.40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17164</th>\n",
       "      <td>2016-12-29</td>\n",
       "      <td>1.392287</td>\n",
       "      <td>11.60</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17165</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>1.308269</td>\n",
       "      <td>10.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17166</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>1.284264</td>\n",
       "      <td>10.70</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17167 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date     layer  discharge  baseflow\n",
       "0     1970-01-01  0.116424       0.97       NaN\n",
       "1     1970-01-02  0.114023       0.95       NaN\n",
       "2     1970-01-03  0.114023       0.95       NaN\n",
       "3     1970-01-04  0.112823       0.94       NaN\n",
       "4     1970-01-05  0.106822       0.89       NaN\n",
       "...          ...       ...        ...       ...\n",
       "17162 2016-12-27  1.440296      12.00       NaN\n",
       "17163 2016-12-28  1.368282      11.40       NaN\n",
       "17164 2016-12-29  1.392287      11.60       NaN\n",
       "17165 2016-12-30  1.308269      10.90       NaN\n",
       "17166 2016-12-31  1.284264      10.70       NaN\n",
       "\n",
       "[17167 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrology_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "domestic-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hydro_signatures_for_CAMELS(hydro_data, path_to_results , ID): # ID IS TEMPORARY\n",
    "    import math\n",
    "    import time\n",
    "    list_of_gauges = hydro_data\n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    def split_by_year(list_of_something, number_of_NaN):\n",
    "    \n",
    "        \"\"\"\n",
    "        Данная функция разбивает лист датафреймов по годам\n",
    "        и выкидывает года, где NaN регулируется переменной number_of_NaN\n",
    "\n",
    "        Колонка Date в датафрейме внутри листа - даты\n",
    "        и она обязательна. \n",
    "        Столбцы могут называться в соответствии с пожеланиями\n",
    "\n",
    "        \"\"\"\n",
    "        splitted_list = list()\n",
    "\n",
    "        for i, gauge in enumerate(list_of_something):\n",
    "            \"\"\"\n",
    "            From list of lists we are calling another list \n",
    "            that consist list of dataframes\n",
    "\n",
    "            \"\"\"\n",
    "            year_split = list()\n",
    "            unique_years = gauge.date.dt.year.unique()\n",
    "\n",
    "            for year in unique_years:\n",
    "                year_split.append(gauge[gauge.date.dt.year == year].reset_index(drop = True))\n",
    "\n",
    "            for i in range(len(year_split)-1, -1, -1):\n",
    "                if sum(year_split[i].discharge.isna()) > number_of_NaN:\n",
    "                    del year_split[i]\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            splitted_list.append(year_split)\n",
    "\n",
    "        return splitted_list\n",
    "\n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    \"\"\"\n",
    "    Q mean for observations on every valid gauge. Dimension - mm/day\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def mean_for_gauge(list_of_gauges):\n",
    "        gauges_Q_mean = [np.nanmean(i.layer) for i in list_of_gauges]\n",
    "        print('Mean layer of discharge calculation')\n",
    "        return gauges_Q_mean\n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    def slope_fdc_gauge(list_of_gauges):\n",
    "        slope_fdc = []\n",
    "        for i, gauge in enumerate(list_of_gauges):\n",
    "            if (np.nanpercentile(gauge.layer.to_numpy(), q = 100 - 33) != 0) & (np.nanpercentile(gauge.layer.to_numpy(), q = 100 - 66) != 0):\n",
    "                slope_fdc.append((math.log(np.nanpercentile(gauge.layer.to_numpy(), q = 100 - 33)) - math.log(np.nanpercentile(gauge.layer.to_numpy(), q = 100 - 66)))/(0.66-0.33))\n",
    "            else:\n",
    "                slope_fdc.append(np.NaN)\n",
    "        print('Flow duration curve calculation')\n",
    "        return slope_fdc\n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    \n",
    "    import numba\n",
    "\n",
    "    \"\"\"\n",
    "    Расчёт ведётся для листа в котором нет пропусков. \n",
    "    В случае их наличия в общем ряду, он разбивается\n",
    "    на n-рядов в зависимости от разбиений\n",
    "\n",
    "    Расчёт BFI далее будет производиться для отдельно \n",
    "    взятого года наблюдений\n",
    "\n",
    "    \"\"\"\n",
    "    ###################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    First pass\n",
    "\n",
    "    \"\"\"\n",
    "    @numba.jit(nopython = True)\n",
    "    def FirstPass(Q, alpha):\n",
    "\n",
    "        q_f_1 = [np.float64(np.NaN) for i in Q]\n",
    "        q_b_1 = [np.float64(np.NaN) for i in Q]\n",
    "\n",
    "        q_f_1[0] = Q[0]\n",
    "\n",
    "        for j in range(len(Q)-1):\n",
    "            \"\"\"\n",
    "            for every split calculate quick flow\n",
    "\n",
    "            \"\"\"\n",
    "            q_f_1[j+1] = alpha * q_f_1[j] + 0.5 * (1 + alpha) * (Q[j+1] - Q[j])\n",
    "\n",
    "        for j in range(len(Q)):\n",
    "            if q_f_1[j] < 0:\n",
    "                q_b_1[j] = Q[j]\n",
    "            else:\n",
    "                q_b_1[j] = Q[j] - q_f_1[j]\n",
    "\n",
    "        Q_forward_1 = [q_f_1, q_b_1]\n",
    "\n",
    "        return Q_forward_1\n",
    "\n",
    "    ###################################################################\n",
    "    \"\"\"\n",
    "    Backward pass\n",
    "\n",
    "    \"\"\"\n",
    "    @numba.jit(nopython = True)\n",
    "    def BackwardPass(Q_forward_1, alpha):\n",
    "\n",
    "        \"\"\"\n",
    "        Здесь Q - n-мерный лист в зависимости от числа разбиений\n",
    "        \"\"\"\n",
    "\n",
    "        Qq = Q_forward_1[0]\n",
    "        Qb = Q_forward_1[1]\n",
    "\n",
    "        q_f_2 = [np.float64(np.NaN) for i in Qq]\n",
    "        q_b_2 = [np.float64(np.NaN) for i in Qb]\n",
    "\n",
    "\n",
    "        \"last value of forward step - first in backward step\"\n",
    "\n",
    "        q_f_2[-1] = Qb[-1]\n",
    "\n",
    "        for j in range(len(Qq)-2, -1, -1):\n",
    "            q_f_2[j] = alpha * q_f_2[j+1] + 0.5 * (1 + alpha) * (Qb[j] - Qb[j+1])\n",
    "\n",
    "        for j in range(len(Qq)-1, -1, -1):\n",
    "            if q_f_2[j] < 0:\n",
    "                q_b_2[j] = Qb[j]\n",
    "            else:\n",
    "                q_b_2[j] = Qb[j] - q_f_2[j]\n",
    "\n",
    "        Q_backward = [q_f_2, q_b_2]\n",
    "\n",
    "        return Q_backward\n",
    "\n",
    "    ###################################################################\n",
    "    \"\"\"\n",
    "    Forward pass\n",
    "\n",
    "    \"\"\"\n",
    "    @numba.jit(nopython = True)\n",
    "    def ForwardPass(Q_backward, alpha):\n",
    "\n",
    "        Qq = Q_backward[0]\n",
    "        Qb = Q_backward[1]\n",
    "\n",
    "        q_f_3 = [np.float64(np.NaN) for i in Qq]\n",
    "        q_b_3 = [np.float64(np.NaN) for i in Qb]\n",
    "\n",
    "\n",
    "        \"Теперь первая величина предыдущего шага - первая и здесь\"\n",
    "\n",
    "        q_f_3[0] = Qb[0]\n",
    "\n",
    "        for j in range(len(Qb)-1):\n",
    "\n",
    "            q_f_3[j+1] = alpha * q_f_3[j] + 0.5 * (1 + alpha) * (Qb[j+1] - Qb[j])\n",
    "\n",
    "        for j in range(len(Qb)):\n",
    "            if q_f_3[j] < 0:\n",
    "                q_b_3[j] = Qb[j]\n",
    "            else:\n",
    "                q_b_3[j] = Qb[j] - q_f_3[j]\n",
    "\n",
    "        Q_forward = [q_f_3, q_b_3]\n",
    "\n",
    "        return Q_forward\n",
    "\n",
    "    ###################################################################\n",
    "    \"\"\"\n",
    "    BFI calculations for given alpha\n",
    "\n",
    "    \"\"\"\n",
    "    @numba.jit(nopython = True)\n",
    "    def BFI_calc(Q, alpha, passes, reflect):\n",
    "        \"\"\"\n",
    "        we reflect the first reflect values and the last reflect values.  \n",
    "        this is to get rid of 'warm up' problems © Anthony Ladson\n",
    "        \"\"\" \n",
    "        Qin = Q\n",
    "\n",
    "        \"reflect our lists\"\n",
    "\n",
    "        if len(Q)-1 > reflect:\n",
    "            Q_reflect = np.array([np.float64(np.NaN) for _ in range(len(Q) + 2 * reflect)], dtype = np.float64)\n",
    "\n",
    "            Q_reflect[:reflect] = Q[(reflect):0:-1]\n",
    "            Q_reflect[(reflect):(reflect + len(Q))] = Q\n",
    "            Q_reflect[(reflect + len(Q)):(len(Q) + 2 + 2 * reflect)] = Q[len(Q)-2:len(Q) - reflect - 2:-1]\n",
    "\n",
    "        else:        \n",
    "            Q_reflect = np.array([np.float64(np.NaN) for _ in range(len(Q))], dtype = np.float64)                 \n",
    "            Q_reflect = Q\n",
    "\n",
    "        Q1 = FirstPass(Q_reflect, alpha)\n",
    "\n",
    "        \"how many backwards/forward passes to we need © Anthony Ladson\"\n",
    "\n",
    "        n_pass = round(0.5 * (passes -1))\n",
    "\n",
    "        BackwardPass(Q1, alpha)\n",
    "\n",
    "        for i in range(n_pass):\n",
    "            Q1 = ForwardPass(BackwardPass(Q1, alpha), alpha)\n",
    "\n",
    "        ################# end of passes  ##############################\n",
    "        if len(Q)-1 > reflect:\n",
    "            Qbase = Q1[1][reflect:(len(Q1[1])-reflect)]\n",
    "            Qbase = [0 if j < 0 else j for j in Qbase]\n",
    "        else:\n",
    "            Qbase = Q1[1]\n",
    "            Qbase = [0 if j < 0 else j for j in Qbase]\n",
    "\n",
    "        bfi = 0\n",
    "        mean_for_period = 0\n",
    "\n",
    "        if np.mean(Qin) == 0:\n",
    "            bfi = 0\n",
    "        else:\n",
    "            for j in Qbase:\n",
    "                mean_for_period += j/np.mean(Qin)\n",
    "            bfi = mean_for_period/len(Qbase)\n",
    "\n",
    "        return bfi, Qbase\n",
    "\n",
    "    \"\"\"\n",
    "    BFI calculations for 1000 alpha between 0.9 and 0.98\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import random\n",
    "    @numba.jit(nopython = True)\n",
    "    def BFI_calc_1000(Q, passes, reflect):\n",
    "        \"\"\"\n",
    "        Расчёт проводится для 1000 случайных значений alpha\n",
    "        в диапазоне он 0.9 до 0.98\n",
    "\n",
    "        we reflect the first reflect values and the last reflect values.  \n",
    "        this is to get rid of 'warm up' problems © Anthony Ladson\n",
    "        \"\"\" \n",
    "\n",
    "        random.seed(1996)\n",
    "        alpha_coefficients = [np.float64(random.uniform(0.9, 0.98)) for i in range(1000)]\n",
    "\n",
    "        Q = np.array([np.float64(i) for i in Q], dtype = np.float64)\n",
    "        Qin = Q\n",
    "\n",
    "        \"reflect our lists\"\n",
    "\n",
    "        if len(Q)-1 > reflect:\n",
    "            Q_reflect = np.array([np.float64(np.NaN) for _ in range(len(Q) + 2 * reflect)], dtype = np.float64)\n",
    "\n",
    "            Q_reflect[:reflect] = Q[(reflect):0:-1]\n",
    "            Q_reflect[(reflect):(reflect + len(Q))] = Q\n",
    "            Q_reflect[(reflect + len(Q)):(len(Q) + 2 + 2 * reflect)] = Q[len(Q)-2:len(Q) - reflect - 2:-1]\n",
    "\n",
    "        else:        \n",
    "            Q_reflect = np.array([np.float64(np.NaN) for _ in range(len(Q))], dtype = np.float64)                 \n",
    "            Q_reflect = Q\n",
    "\n",
    "        bfi_record = []\n",
    "        Qbase_record = []\n",
    "\n",
    "        for i, alpha in enumerate(alpha_coefficients):\n",
    "\n",
    "            Q1 = FirstPass(Q_reflect, alpha)\n",
    "\n",
    "            \"how many backwards/forward passes to we need © Anthony Ladson\"\n",
    "\n",
    "            n_pass = round(0.5 * (passes -1))\n",
    "\n",
    "            BackwardPass(Q1, alpha)\n",
    "\n",
    "            for i in range(n_pass):\n",
    "                Q1 = ForwardPass(BackwardPass(Q1, alpha), alpha)\n",
    "\n",
    "\n",
    "            ################# end of passes  ##############################\n",
    "            if len(Q)-1 > reflect:\n",
    "                Qbase = Q1[1][reflect:(len(Q1[1])-reflect)]\n",
    "                Qbase = [0 if j < 0 else j for j in Qbase]\n",
    "            else:\n",
    "                Qbase = Q1[1]\n",
    "                Qbase = [0 if j < 0 else j for j in Qbase]\n",
    "\n",
    "            Qbase_record.append(np.array(Qbase, dtype=np.float64))\n",
    "\n",
    "            bfi = 0\n",
    "            mean_for_period = 0\n",
    "\n",
    "            if np.mean(Qin) == 0:\n",
    "                bfi = 0\n",
    "            else:\n",
    "                for j in Qbase:\n",
    "                    mean_for_period += j/np.mean(Qin)\n",
    "                bfi = mean_for_period/len(Qbase)\n",
    "\n",
    "            bfi_record.append(np.float64(bfi))\n",
    "\n",
    "        \"\"\"\n",
    "        After 1000 calculations function return\n",
    "        mean value out of 1000\n",
    "\n",
    "        And \"mean\" hygrograph of baseflow\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # mean BFI out of 1000\n",
    "\n",
    "        bfi_mean = 0\n",
    "        for i in bfi_record:\n",
    "            bfi_mean += i\n",
    "        bfi_mean = bfi_mean/len(bfi_record)\n",
    "\n",
    "        # mean hydrograph out of 1000 calculations\n",
    "\n",
    "        Qbase_mean = [np.float64(0) for i in range(len(Qbase))]\n",
    "\n",
    "        for Qbase_temp in Qbase_record:\n",
    "            for i, value in enumerate(Qbase_temp):\n",
    "                Qbase_mean[i] += value\n",
    "\n",
    "        Qbase_mean = [np.float64(i/len(Qbase_record)) for i in Qbase_mean]\n",
    "        \n",
    "        return bfi_mean, Qbase_mean\n",
    "    \n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    def clump_array(a):\n",
    "\n",
    "        \"\"\"\n",
    "        Разбить период наблюдений на куски, в которых нет NaN\n",
    "        \"\"\"\n",
    "\n",
    "        return [np.float64(a[s]) for s in np.ma.clump_unmasked(np.ma.masked_invalid(a))]\n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    \n",
    "    def BFI_calculation_for_lists(Valid_gauges_Q_cms, Number_of_NaN):\n",
    "        # \n",
    "        every_gauge_split_by_year = split_by_year(Valid_gauges_Q_cms, Number_of_NaN)\n",
    "        # разбитие столбца из датафрейма на промежутки без NaN\n",
    "        every_gauge_split_by_year_np = [[clump_array(year.discharge.to_numpy()) for year in gauge] for gauge in every_gauge_split_by_year]\n",
    "\n",
    "        clump_bfi = [[[[] for clump in year]  for year in gauge] for gauge in every_gauge_split_by_year_np]\n",
    "        weights_of_clump = [[[[] for clump in year]  for year in gauge] for gauge in every_gauge_split_by_year_np]\n",
    "        clump_Qbase = [[[[] for clump in year]  for year in gauge] for gauge in every_gauge_split_by_year_np]\n",
    "        \n",
    "        print('Base flow index calculation')\n",
    "        for i, gauge in enumerate(every_gauge_split_by_year_np):\n",
    "            start = time.time()\n",
    "            for j, year in enumerate(gauge):\n",
    "                for k, clump in enumerate(year):\n",
    "                    # проверка, что кусок между NaN больше, длины сравнения\n",
    "                    if type(clump) == np.ndarray:\n",
    "                        clump_bfi[i][j][k] = BFI_calc_1000(clump, 3, 30)[0]\n",
    "                        weights_of_clump[i][j][k] = len(clump)/len(every_gauge_split_by_year[i][j].discharge)\n",
    "                        clump_Qbase[i][j][k] = BFI_calc_1000(clump, 3, 30)[1]\n",
    "                    else:\n",
    "                        clump_bfi[i][j][k] = np.NaN\n",
    "                        weights_of_clump[i][j][k] = np.NaN\n",
    "                        clump_Qbase[i][j][k] = [np.NaN]\n",
    "            stop = time.time()\n",
    "            print('Расчёт на {} посту закончилась за {} секунд'.format(i, round(stop - start)))\n",
    "        weighted_bfi = [np.mean([np.sum([bfi * weights_of_clump[i][j][k] for k, bfi in enumerate(year)]) for j, year in enumerate(gauge)]) if len(gauge) > 1 else np.NaN for i, gauge in enumerate(clump_bfi)]\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "           Заполнение столбца baseflow восстанавливая разбитые на куски без NaN наблюдения за стоком\n",
    "        \"\"\"\n",
    "\n",
    "        from itertools import groupby\n",
    "        def bfi_recover(year_of_observation, bfi_values):\n",
    "\n",
    "            \"\"\"\n",
    "            Восстановления ряда baseflow с учётом ранее \"выбитых\" пропусков\n",
    "            \"\"\"\n",
    "            Qbase_full = [list(group) for key, group in groupby(year_of_observation.discharge.to_numpy(), key=np.isnan)] # разбитие на группы по признаку. Получается n листов. NaN и !NaN разбиты по листам\n",
    "            mask_to_refill = [~np.isnan(i).any() for i in Qbase_full] # индексируем их как одинокие булевые величины\n",
    "\n",
    "            position_index = -1\n",
    "            for j, mask in enumerate(mask_to_refill):\n",
    "                \"\"\"\n",
    "                Так как первичный разрезанный лист из функции clumped_array имеет в себе количество листов, такое же как и True, то восстанавливаем в соответствии\n",
    "\n",
    "                Аргумент position_index подбирается в соответствии с присутсвием с True и адресуется к нужному куску в Qbase\n",
    "\n",
    "                Всё перезаписывается в лист, где в перезаписываемых значениях эквивалентные по длине значения стока\n",
    "                \"\"\"\n",
    "                if len(Qbase_full) > 1:\n",
    "                    if mask:\n",
    "                        position_index += 1\n",
    "                        Qbase_full[j] = bfi_values[position_index]\n",
    "                    else:\n",
    "                        position_index = position_index\n",
    "                        Qbase_full[j] = Qbase_full[j]\n",
    "                else:\n",
    "                    Qbase_full[j] = [item for sublist in bfi_values for item in sublist]\n",
    "\n",
    "            if len(Qbase_full) > 1:\n",
    "                Qbase_full = [item for sublist in Qbase_full for item in sublist] # лист листов в лист с учётом NaN\n",
    "            else:\n",
    "                Qbase_full = [item for sublist in Qbase_full for item in sublist]\n",
    "\n",
    "            return Qbase_full\n",
    "\n",
    "        for i, gauge in enumerate(every_gauge_split_by_year_np):\n",
    "            for j, year in enumerate(gauge):\n",
    "                for k, clump in enumerate(year):\n",
    "                    every_gauge_split_by_year[i][j].loc[:, 'baseflow'] = bfi_recover(every_gauge_split_by_year[i][j], clump_Qbase[i][j])\n",
    "        print('Base Flow Index splitting')\n",
    "        return weighted_bfi, every_gauge_split_by_year\n",
    "    \n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    \n",
    "    def hfd_mean_for_gauges(Valid_gauges_cms, number_of_NaN):\n",
    "        \n",
    "        hydro_data = Valid_gauges_cms\n",
    "        import datetime\n",
    "\n",
    "        every_gauge_split_by_year_hfd = split_by_year(hydro_data, number_of_NaN)\n",
    "\n",
    "        hfd = [[[] for year in range(len(gauge)-1)] for gauge in every_gauge_split_by_year_hfd]\n",
    "\n",
    "        for i, gauge in enumerate(every_gauge_split_by_year_hfd):\n",
    "            for j in range(len(gauge)-1):\n",
    "\n",
    "                if (datetime.datetime(gauge[0].date[0].year, 10, 1) >= gauge[0].date[0]) & \\\n",
    "                (len(gauge[0]) > 91): # check if first split is starting at 1 of october or earlier and have no spaces between 1.10.i and 31.12.i\n",
    "\n",
    "                    # if it's suffice criteria then we'll split it by hydrological years: two consecutive years from 1.10.i tp 1.10.i+1\n",
    "                    hfd[i][j] = pd.concat([gauge[j][gauge[j].date >= datetime.datetime(gauge[j].date[0].year, 10, 1)], \n",
    "                       gauge[j+1][gauge[j+1].date < datetime.datetime(gauge[j+1].date[0].year, 10, 1)]]).reset_index(drop = True)\n",
    "                else:\n",
    "                    hfd[i][j].append(np.NaN)\n",
    "\n",
    "        hfd_value = [[] for _ in every_gauge_split_by_year_hfd]\n",
    "\n",
    "        for i, gauge in enumerate(hfd):\n",
    "            for j, hydrological_year in enumerate(gauge):\n",
    "                half_discharge = 0\n",
    "                half_sum = gauge[j].discharge.dropna().sum()/2\n",
    "\n",
    "                for k, discharge in enumerate(hydrological_year.discharge):\n",
    "                    half_discharge += discharge\n",
    "                    if half_discharge > half_sum:\n",
    "                        hfd_value[i].append(k + 1)\n",
    "                        break\n",
    "        hfd_value = [np.nanmean(gauge_dates) for gauge_dates in hfd_value]\n",
    "\n",
    "        print('Half flow date calculation')\n",
    "\n",
    "        return hfd_value\n",
    "        \n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    \n",
    "    def Q5_for_gauge(list_of_gauges):\n",
    "        Q5 = [np.nanpercentile(gauge.layer.to_numpy(), q = 5) for gauge in list_of_gauges]\n",
    "        print('5% quantile calculation')\n",
    "        return Q5\n",
    "    \n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    \n",
    "    def Q95_for_gauge(list_of_gauges):\n",
    "        Q95 = [np.nanpercentile(gauge.layer.to_numpy(), q = 95) for gauge in list_of_gauges]\n",
    "        print('95% quantile calculation')\n",
    "        return Q95\n",
    "    \n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    \n",
    "    def high_q_frequency(Valid_gauges_in_mm): #Anthony Ladson version\n",
    "    \n",
    "        hydro_data = Valid_gauges_in_mm\n",
    "\n",
    "        high_q_f = list()\n",
    "\n",
    "        for gauge in hydro_data:\n",
    "            if len(gauge) > 1:\n",
    "                high_q_f.append(\n",
    "                len(gauge.layer.dropna()[gauge.layer.dropna() > gauge.layer.dropna().median() * 9])/len(gauge.layer.dropna()) * 365.25\n",
    "                )\n",
    "            else:\n",
    "                high_q_f.append(np.NaN)\n",
    "\n",
    "        print('High discharge frequency calculation')\n",
    "\n",
    "        return high_q_f    \n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    \n",
    "    def high_q_duration(Valid_gauges_in_mm, number_of_Nan):    \n",
    "        hydro_data = Valid_gauges_in_mm\n",
    "\n",
    "        every_gauge_split_by_year_mm = split_by_year(hydro_data, number_of_Nan)\n",
    "\n",
    "        FREQ_INSTANCE = list()\n",
    "        SEQ_OF_FREQ = list()\n",
    "        LEN_OF_SEQ = [[[] for year in gauge] for gauge in every_gauge_split_by_year_mm]\n",
    "\n",
    "\n",
    "        for i, gauge in enumerate(every_gauge_split_by_year_mm):\n",
    "            median_x9 = hydro_data[i].layer.dropna().median() * 9\n",
    "            FREQ_INSTANCE.append([every_gauge_split_by_year_mm[i][j][every_gauge_split_by_year_mm[i][j].layer > median_x9] \n",
    "                              for j in range(len(every_gauge_split_by_year_mm[i]))])\n",
    "\n",
    "            SEQ_OF_FREQ.append([[d for _, d in FREQ_INSTANCE[i][j].groupby(FREQ_INSTANCE[i][j].index - np.arange(len(FREQ_INSTANCE[i][j])))]\n",
    "                                if FREQ_INSTANCE[i][j].empty != True else [0]\n",
    "                           for j in range(len(FREQ_INSTANCE[i]))])\n",
    "            for k, seq in enumerate(SEQ_OF_FREQ[i]):\n",
    "                if len(seq) == 0:\n",
    "                    LEN_OF_SEQ[i][k] = [np.NaN]\n",
    "                else:\n",
    "                    LEN_OF_SEQ[i][k] = [len(seq[ii])\n",
    "                                        if type(seq[ii]) != int\n",
    "                                        else 0\n",
    "                                        for ii in range(len(seq))]\n",
    "        high_q_d = list()\n",
    "\n",
    "        for i, gauge in enumerate(LEN_OF_SEQ):\n",
    "            if len(gauge) > 1:\n",
    "                if all([True if i == 0 else False for i in [item for sublist in LEN_OF_SEQ[i] for item in sublist] ]):\n",
    "                    high_q_d.append(0)\n",
    "                else:\n",
    "                    high_q_d.append(np.nanmean([i if i != 0 else np.NaN for i in [item for sublist in LEN_OF_SEQ[i] for item in sublist]]))\n",
    "            else:\n",
    "                high_q_d.append(np.NaN)\n",
    "\n",
    "        print('High discharge duration calculation')\n",
    "\n",
    "        return high_q_d\n",
    "    \n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    \n",
    "    def low_q_frequency(Valid_gauges_in_mm): #Anthony Ladson version\n",
    "    \n",
    "        hydro_data = Valid_gauges_in_mm\n",
    "\n",
    "        low_q_f = list()\n",
    "\n",
    "        for gauge in hydro_data:\n",
    "            if len(gauge) > 1:\n",
    "                low_q_f.append(\n",
    "                len(gauge.layer.dropna()[gauge.layer.dropna() < gauge.layer.dropna().mean() * 0.2])/len(gauge.layer.dropna()) * 365.25\n",
    "                )\n",
    "            else:\n",
    "                low_q_f.append(np.NaN)\n",
    "\n",
    "        print('High discharge frequency calculation')\n",
    "\n",
    "        return low_q_f\n",
    "    \n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    \n",
    "    def low_q_duration(Valid_gauges_in_mm, number_of_Nan):\n",
    "    \n",
    "        hydro_data = Valid_gauges_in_mm\n",
    "\n",
    "        every_gauge_split_by_year_mm = split_by_year(hydro_data, number_of_Nan)\n",
    "\n",
    "        FREQ_INSTANCE = list()\n",
    "        SEQ_OF_FREQ = list()\n",
    "        LEN_OF_SEQ = [[[] for year in gauge] for gauge in every_gauge_split_by_year_mm]\n",
    "\n",
    "\n",
    "        for i, gauge in enumerate(every_gauge_split_by_year_mm):\n",
    "            mean_x02 = hydro_data[i].layer.dropna().mean() * 0.2\n",
    "            FREQ_INSTANCE.append([every_gauge_split_by_year_mm[i][j][every_gauge_split_by_year_mm[i][j].layer < mean_x02] \n",
    "                              for j in range(len(every_gauge_split_by_year_mm[i]))])\n",
    "\n",
    "            SEQ_OF_FREQ.append([[d for _, d in FREQ_INSTANCE[i][j].groupby(FREQ_INSTANCE[i][j].index - np.arange(len(FREQ_INSTANCE[i][j])))]\n",
    "                                if FREQ_INSTANCE[i][j].empty != True else [0]\n",
    "                           for j in range(len(FREQ_INSTANCE[i]))])\n",
    "            for k, seq in enumerate(SEQ_OF_FREQ[i]):\n",
    "                if len(seq) == 0:\n",
    "                    LEN_OF_SEQ[i][k] = [np.NaN]\n",
    "                else:\n",
    "                    LEN_OF_SEQ[i][k] = [len(seq[ii])\n",
    "                                        if type(seq[ii]) != int\n",
    "                                        else 0\n",
    "                                        for ii in range(len(seq))]\n",
    "        low_q_d = list()\n",
    "\n",
    "\n",
    "        for i, gauge in enumerate(LEN_OF_SEQ):\n",
    "            if len(gauge) > 1:\n",
    "                if all([True if i == 0 else False for i in [item for sublist in LEN_OF_SEQ[i] for item in sublist]]):\n",
    "                    low_q_d.append(0)\n",
    "                else:\n",
    "                    low_q_d.append(np.nanmean([i if i != 0 else np.NaN for i in [item for sublist in LEN_OF_SEQ[i] for item in sublist]]))\n",
    "            else:\n",
    "                low_q_d.append(np.NaN)\n",
    "\n",
    "        print('Low discharge duration calculation')\n",
    "\n",
    "        return low_q_d\n",
    "\n",
    "    \n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "    \n",
    "    \n",
    "    def zero_q_frequency(Valid_gauges_in_mm):\n",
    "        \n",
    "        hydro_data = Valid_gauges_in_mm\n",
    "\n",
    "        zero_q_freq = list()\n",
    "\n",
    "        for gauge in hydro_data:\n",
    "            if len(gauge) > 1:\n",
    "                zero_q_freq.append(\n",
    "                len(gauge.layer.dropna()[gauge.layer.dropna() == 0.0])/len(gauge.layer.dropna()) * 365.25\n",
    "                )\n",
    "            else:\n",
    "                zero_q_freq.append(np.NaN)\n",
    "\n",
    "        print('Zero discharge frequency')\n",
    "        return zero_q_freq    \n",
    "    \n",
    "    ###########################################################################################################################################\n",
    "\n",
    "    camels_ru_hydro = pd.DataFrame(data = [ID, \n",
    "                                           mean_for_gauge(list_of_gauges),\n",
    "                                           slope_fdc_gauge(list_of_gauges),\n",
    "                                           BFI_calculation_for_lists(list_of_gauges, 60)[0],\n",
    "                                           Q5_for_gauge(list_of_gauges), \n",
    "                                           Q95_for_gauge(list_of_gauges),\n",
    "                                           high_q_frequency(list_of_gauges), #my require 0\n",
    "                                           high_q_duration(list_of_gauges, 0), \n",
    "                                           low_q_frequency(list_of_gauges), #my require 0\n",
    "                                           low_q_duration(list_of_gauges, 0),\n",
    "                                           zero_q_frequency(list_of_gauges),\n",
    "                                           hfd_mean_for_gauges(list_of_gauges, 0)])\n",
    "    camels_ru_hydro = camels_ru_hydro.T\n",
    "    \n",
    "    camels_ru_hydro.columns = ['gauge_id', 'q_mean', 'slope_fdc', 'baseflow_index', 'q5', 'q95',\n",
    "                               'high_q_freq', 'high_q_dur', 'low_q_freq', 'low_q_dur', 'zero_q_freq', 'hfd_mean']\n",
    "    camels_ru_hydro = camels_ru_hydro.applymap('{:.4f}'.format)\n",
    "    camels_ru_hydro['gauge_id'] = ID\n",
    "#     camels_ru_hydro.to_csv(path_to_results + 'camels_ru_hydro.csv', sep = ';', index = False)\n",
    "    \n",
    "    return camels_ru_hydro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lyric-healing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean layer of discharge calculation\n",
      "Flow duration curve calculation\n",
      "Base flow index calculation\n",
      "Расчёт на 0 посту закончилась за 13 секунд\n",
      "Расчёт на 1 посту закончилась за 11 секунд\n",
      "Расчёт на 2 посту закончилась за 9 секунд\n",
      "Расчёт на 3 посту закончилась за 11 секунд\n",
      "Расчёт на 4 посту закончилась за 11 секунд\n",
      "Расчёт на 5 посту закончилась за 12 секунд\n",
      "Расчёт на 6 посту закончилась за 9 секунд\n",
      "Расчёт на 7 посту закончилась за 7 секунд\n",
      "Расчёт на 8 посту закончилась за 11 секунд\n",
      "Расчёт на 9 посту закончилась за 11 секунд\n",
      "Расчёт на 10 посту закончилась за 6 секунд\n",
      "Base Flow Index splitting\n",
      "5% quantile calculation\n",
      "95% quantile calculation\n",
      "High discharge frequency calculation\n",
      "High discharge duration calculation\n",
      "High discharge frequency calculation\n",
      "Low discharge duration calculation\n",
      "Zero discharge frequency\n",
      "Half flow date calculation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gauge_id</th>\n",
       "      <th>q_mean</th>\n",
       "      <th>slope_fdc</th>\n",
       "      <th>baseflow_index</th>\n",
       "      <th>q5</th>\n",
       "      <th>q95</th>\n",
       "      <th>high_q_freq</th>\n",
       "      <th>high_q_dur</th>\n",
       "      <th>low_q_freq</th>\n",
       "      <th>low_q_dur</th>\n",
       "      <th>zero_q_freq</th>\n",
       "      <th>hfd_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72552</td>\n",
       "      <td>0.5039</td>\n",
       "      <td>2.6141</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>1.7404</td>\n",
       "      <td>7.8856</td>\n",
       "      <td>4.9394</td>\n",
       "      <td>41.4106</td>\n",
       "      <td>16.5524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>180.0750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72559</td>\n",
       "      <td>1.0262</td>\n",
       "      <td>2.1093</td>\n",
       "      <td>0.6024</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>2.9562</td>\n",
       "      <td>1.5654</td>\n",
       "      <td>3.1818</td>\n",
       "      <td>9.3487</td>\n",
       "      <td>9.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>177.2326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72584</td>\n",
       "      <td>0.3761</td>\n",
       "      <td>3.3685</td>\n",
       "      <td>0.6874</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>83.4214</td>\n",
       "      <td>37.8667</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>217.6562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72585</td>\n",
       "      <td>1.2275</td>\n",
       "      <td>1.3142</td>\n",
       "      <td>0.7893</td>\n",
       "      <td>0.4539</td>\n",
       "      <td>2.4685</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0741</td>\n",
       "      <td>12.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>204.9070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72588</td>\n",
       "      <td>0.8675</td>\n",
       "      <td>1.3347</td>\n",
       "      <td>0.6903</td>\n",
       "      <td>0.3128</td>\n",
       "      <td>2.1252</td>\n",
       "      <td>1.5111</td>\n",
       "      <td>6.1818</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>196.6744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72592</td>\n",
       "      <td>0.7401</td>\n",
       "      <td>2.5051</td>\n",
       "      <td>0.5408</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>2.4317</td>\n",
       "      <td>8.0229</td>\n",
       "      <td>8.3864</td>\n",
       "      <td>34.0703</td>\n",
       "      <td>25.2742</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>189.9545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>72603</td>\n",
       "      <td>0.5292</td>\n",
       "      <td>3.4358</td>\n",
       "      <td>0.4492</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>2.0127</td>\n",
       "      <td>18.2449</td>\n",
       "      <td>7.4176</td>\n",
       "      <td>97.9550</td>\n",
       "      <td>29.6435</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>188.4286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>72605</td>\n",
       "      <td>0.6734</td>\n",
       "      <td>3.1410</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>2.5795</td>\n",
       "      <td>15.9856</td>\n",
       "      <td>5.9178</td>\n",
       "      <td>68.5611</td>\n",
       "      <td>14.0075</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>191.9615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>72610</td>\n",
       "      <td>1.6280</td>\n",
       "      <td>2.1163</td>\n",
       "      <td>0.5954</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>4.9268</td>\n",
       "      <td>3.1421</td>\n",
       "      <td>5.2593</td>\n",
       "      <td>10.9652</td>\n",
       "      <td>6.7143</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>184.5238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>72617</td>\n",
       "      <td>1.1986</td>\n",
       "      <td>2.0741</td>\n",
       "      <td>0.6634</td>\n",
       "      <td>0.2841</td>\n",
       "      <td>3.1755</td>\n",
       "      <td>0.1137</td>\n",
       "      <td>2.5000</td>\n",
       "      <td>10.5013</td>\n",
       "      <td>10.4048</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>188.0476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>72729</td>\n",
       "      <td>0.8734</td>\n",
       "      <td>1.3059</td>\n",
       "      <td>0.7511</td>\n",
       "      <td>0.2982</td>\n",
       "      <td>1.8922</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.5797</td>\n",
       "      <td>25.2000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>192.3500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gauge_id  q_mean slope_fdc baseflow_index      q5     q95 high_q_freq  \\\n",
       "0      72552  0.5039    2.6141         0.4992  0.0684  1.7404      7.8856   \n",
       "1      72559  1.0262    2.1093         0.6024  0.2383  2.9562      1.5654   \n",
       "2      72584  0.3761    3.3685         0.6874  0.0058  0.9847      0.0000   \n",
       "3      72585  1.2275    1.3142         0.7893  0.4539  2.4685      0.0000   \n",
       "4      72588  0.8675    1.3347         0.6903  0.3128  2.1252      1.5111   \n",
       "5      72592  0.7401    2.5051         0.5408  0.1232  2.4317      8.0229   \n",
       "6      72603  0.5292    3.4358         0.4492  0.0503  2.0127     18.2449   \n",
       "7      72605  0.6734    3.1410         0.4700  0.0899  2.5795     15.9856   \n",
       "8      72610  1.6280    2.1163         0.5954  0.3636  4.9268      3.1421   \n",
       "9      72617  1.1986    2.0741         0.6634  0.2841  3.1755      0.1137   \n",
       "10     72729  0.8734    1.3059         0.7511  0.2982  1.8922      0.0000   \n",
       "\n",
       "   high_q_dur low_q_freq low_q_dur zero_q_freq  hfd_mean  \n",
       "0      4.9394    41.4106   16.5524      0.0000  180.0750  \n",
       "1      3.1818     9.3487    9.2500      0.0000  177.2326  \n",
       "2      0.0000    83.4214   37.8667      0.1209  217.6562  \n",
       "3      0.0000     1.0741   12.2500      0.0000  204.9070  \n",
       "4      6.1818     0.0222    1.0000      0.0000  196.6744  \n",
       "5      8.3864    34.0703   25.2742      0.0000  189.9545  \n",
       "6      7.4176    97.9550   29.6435      0.0000  188.4286  \n",
       "7      5.9178    68.5611   14.0075      0.0000  191.9615  \n",
       "8      5.2593    10.9652    6.7143      0.0000  184.5238  \n",
       "9      2.5000    10.5013   10.4048      0.0000  188.0476  \n",
       "10     0.0000     5.5797   25.2000      0.0000  192.3500  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydro_signatures_for_CAMELS(hydro_data = hydrology_data, path_to_results = path_to_results + 'test', ID = ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "senior-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_HydroATLAS_for_WS(WS, WS_index, path_to_HydroATLAS):\n",
    "    \"\"\"                             \n",
    "    WS - Your data with watershed GDF.geometry[:]\n",
    "    WS_index - Number of the index of WS from GeoDataFrame geometry field\n",
    "    path_to_HydroATLAS - Path to BasinATLAS_v10.gdb file\n",
    "    path_to_results - Path where calculations will be saved\n",
    "    \"\"\"\n",
    "    import fiona \n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    def polygon_area(lats, lons, radius = 6378137):\n",
    "        \"\"\"\n",
    "        Computes area of spherical polygon, assuming spherical Earth. \n",
    "        Returns result in ratio of the sphere's area if the radius is specified.\n",
    "        Otherwise, in the units of provided radius.\n",
    "        lats and lons are in degrees.\n",
    "        \"\"\"\n",
    "        from numpy import arctan2, cos, sin, sqrt, pi, power, append, diff, deg2rad\n",
    "        lats = np.deg2rad(lats)\n",
    "        lons = np.deg2rad(lons)\n",
    "\n",
    "        # Line integral based on Green's Theorem, assumes spherical Earth\n",
    "\n",
    "        #close polygon\n",
    "        if lats[0]!=lats[-1]:\n",
    "            lats = append(lats, lats[0])\n",
    "            lons = append(lons, lons[0])\n",
    "\n",
    "        #colatitudes relative to (0,0)\n",
    "        a = sin(lats/2)**2 + cos(lats)* sin(lons/2)**2\n",
    "        colat = 2*arctan2( sqrt(a), sqrt(1-a) )\n",
    "\n",
    "        #azimuths relative to (0,0)\n",
    "        az = arctan2(cos(lats) * sin(lons), sin(lats)) % (2*pi)\n",
    "\n",
    "        # Calculate diffs\n",
    "        # daz = diff(az) % (2*pi)\n",
    "        daz = diff(az)\n",
    "        daz = (daz + pi) % (2 * pi) - pi\n",
    "\n",
    "        deltas=diff(colat)/2\n",
    "        colat=colat[0:-1]+deltas\n",
    "\n",
    "        # Perform integral\n",
    "        integrands = (1-cos(colat)) * daz\n",
    "\n",
    "        # Integrate \n",
    "        area = abs(sum(integrands))/(4*pi)\n",
    "\n",
    "        area = min(area,1-area)\n",
    "        if radius is not None: #return in units of radius\n",
    "            return area * 4 * pi* radius**2 / 10**6\n",
    "        else: #return in ratio of sphere total area\n",
    "            return area / 10**6\n",
    "\n",
    "    def select_big_from_MP(WS_geometry):\n",
    "        if type(WS_geometry) == MultiPolygon:\n",
    "            big_area = [polygon_area(lats = polygon.exterior.coords.xy[1], \n",
    "                                    lons = polygon.exterior.coords.xy[0]) \n",
    "                        for polygon in WS_geometry]\n",
    "            import numpy as np\n",
    "            WS_geometry = WS_geometry[np.argmax(big_area)]\n",
    "        else:\n",
    "            WS_geometry = WS_geometry\n",
    "        return WS_geometry\n",
    "    \n",
    "    # basic numbers for different variables\n",
    "    monthes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    land_cover_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22']\n",
    "    natural_vegetation = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
    "    wetland_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09']\n",
    "    \n",
    "    # Get all the layers from the .gdb file \n",
    "    layers = fiona.listlayers(path_to_HydroATLAS)[-1]\n",
    "    # -1 layer - high density sub-basins (lowest area)\n",
    "    \n",
    "    # Read choosen geodatabase layer with geopandas\n",
    "    gdf = gpd.read_file(path_to_HydroATLAS, \n",
    "                        mask = WS.geometry[WS_index], layer=layers,  ignore_geometry=False)\n",
    "    \n",
    "    def filter_HydroATLAS_sub_basins(WS_own, HydroATLAS_data):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        WS_own - Watershed from your GDF of watersheds\n",
    "        HydroATLAS_data - gdf file from layers of geodatabase\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # return biggest polygon from multipolygon array\n",
    "\n",
    "        def select_big_from_MP(WS_geometry):\n",
    "            if type(WS_geometry) == MultiPolygon:\n",
    "                big_area = [polygon_area(lats = polygon.exterior.coords.xy[1], \n",
    "                                        lons = polygon.exterior.coords.xy[0]) \n",
    "                            for polygon in WS_geometry]\n",
    "                import numpy as np\n",
    "                WS_geometry = WS_geometry[np.argmax(big_area)]\n",
    "            else:\n",
    "                WS_geometry = WS_geometry\n",
    "            return WS_geometry\n",
    "\n",
    "        WS_own = select_big_from_MP(WS_own)\n",
    "        ### WS from your data\n",
    "        your_WS = gpd.GeoSeries([WS_own])\n",
    "\n",
    "        ### Create extra gdf to use geopandas functions\n",
    "        gdf_your_WS = gpd.GeoDataFrame({'geometry': your_WS})\n",
    "        gdf_your_WS = gdf_your_WS.set_crs('EPSG:4326')\n",
    "\n",
    "        intersected_sub_basins = list()\n",
    "\n",
    "\n",
    "\n",
    "        for HydroATLAS_row in range(len(HydroATLAS_data)):\n",
    "\n",
    "            # selection from sub-basins of GeoDataBase\n",
    "            HydroATLAS_WS = gpd.GeoSeries([HydroATLAS_data.geometry[HydroATLAS_row]])        \n",
    "\n",
    "            gdf_HydroATLAS_WS = gpd.GeoDataFrame({'geometry': HydroATLAS_WS}).set_crs('EPSG:4326')\n",
    "\n",
    "            #intersect basins\n",
    "            res_intersection = gpd.overlay(gdf_your_WS, gdf_HydroATLAS_WS, how='intersection')\n",
    "\n",
    "            \"\"\"\n",
    "            Check if our intersection between sub-basin form HydroAtlas and our watershed is more than 0.6 of \n",
    "            sub-basin itself\n",
    "            If not - than pass        \n",
    "            \"\"\"\n",
    "            if len(res_intersection) != 0:\n",
    "                # check if it is MultiPolygon\n",
    "                if type(res_intersection.geometry[0]) == MultiPolygon:\n",
    "                    if polygon_area(lats = res_intersection.geometry[0][0].exterior.coords.xy[1], \n",
    "                                    lons = res_intersection.geometry[0][0].exterior.coords.xy[0])/polygon_area(lats = gdf_HydroATLAS_WS.geometry[0][0].exterior.coords.xy[1], \n",
    "                                                                                                    lons = gdf_HydroATLAS_WS.geometry[0][0].exterior.coords.xy[0]) > 0.1:\n",
    "\n",
    "\n",
    "                        intersected_sub_basins.append(HydroATLAS_data.loc[HydroATLAS_row])\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "                # If it's not it is Polygon\n",
    "                else:\n",
    "                    if polygon_area(lats = res_intersection.geometry[0].exterior.coords.xy[1], \n",
    "                                    lons = res_intersection.geometry[0].exterior.coords.xy[0])/polygon_area(lats = gdf_HydroATLAS_WS.geometry[0][0].exterior.coords.xy[1], \n",
    "                                                                                                    lons = gdf_HydroATLAS_WS.geometry[0][0].exterior.coords.xy[0]) > 0.1:\n",
    "\n",
    "\n",
    "                        intersected_sub_basins.append(HydroATLAS_data.loc[HydroATLAS_row])\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return intersected_sub_basins\n",
    "    \n",
    "    list_of_goodies = filter_HydroATLAS_sub_basins(WS.geometry[WS_index], gdf)\n",
    "  \n",
    "    if len(list_of_goodies) != 0:\n",
    "        list_of_goodies = gpd.GeoDataFrame(pd.DataFrame(list_of_goodies)).set_crs('EPSG:4326').reset_index(drop = True)\n",
    "        from shapely.ops import unary_union\n",
    "        union_geometry = gpd.GeoSeries(unary_union([i for i in list_of_goodies.geometry])).set_crs('epsg:4326')\n",
    "        union_geometry = select_big_from_MP(union_geometry.geometry[0])\n",
    "\n",
    "        \"\"\"\n",
    "        group columns by category (difference is the way of mathematical aggregation)\n",
    "\n",
    "        e.g. classes will be aggrgated by mode value in sub-basins of the watershed\n",
    "\n",
    "        other values will be calculated as a mean for selected watershed\n",
    "\n",
    "        \"\"\"\n",
    "        # values which will be aggregated by mean\n",
    "        columns_MEAN = [['inu_pc_ult'], ['lka_pc_use'], ['lkv_mc_usu'], ['rev_mc_usu'], ['dor_pc_pva'], ['gwt_cm_sav'], ['ele_mt_sav'], ['slp_dg_sav'],\n",
    "                ['sgr_dk_sav'], ['tmp_dc_s{}'.format(i) for i in monthes], ['pre_mm_s{}'.format(i) for i in monthes], \n",
    "                ['pet_mm_s{}'.format(i) for i in monthes], ['aet_mm_s{}'.format(i) for i in monthes], ['snw_pc_s{}'.format(i) for i in monthes], \n",
    "                ['glc_pc_s{}'.format(i) for i in land_cover_classes], ['pnv_pc_s{}'.format(i) for i in natural_vegetation], ['wet_pc_s{}'.format(i) for i in wetland_classes], \n",
    "                ['for_pc_sse'], ['crp_pc_sse'], ['pst_pc_sse'], ['ire_pc_sse'], ['gla_pc_sse'], ['prm_pc_sse'], ['cly_pc_sav'], ['slt_pc_sav'], \n",
    "                ['snd_pc_sav'], ['soc_th_sav'], ['swc_pc_syr'], ['swc_pc_s{}'.format(i) for i in monthes], ['kar_pc_sse'], ['ero_kh_sav'], ['urb_pc_sse']]\n",
    "\n",
    "        # values which will be aggregated by mode\n",
    "        columns_MODE = [['clz_cl_smj'], ['cls_cl_smj'], ['glc_cl_smj'], ['pnv_cl_smj'],\n",
    "                        ['wet_cl_smj'], ['tbi_cl_smj'], ['tec_cl_smj'], ['lit_cl_smj']]\n",
    "\n",
    "        # values which will be aggregated by mean but need extra calculations\n",
    "        # e.g. ari_ix need to be divided by 10, cmi by 100 etc.\n",
    "        # for some reason values exceed treshold of the range\n",
    "        columns_EXTRA_CALC_MEAN = [['ari_ix_sav'], ['cmi_ix_s{}'.format(i) for i in monthes], ['hft_ix_s93'], ['hft_ix_s09']]\n",
    "\n",
    "        # split list of lists to needed columns\n",
    "        columns_EXTRA_CALC_MEAN = [item for sublist in columns_EXTRA_CALC_MEAN for item in sublist]\n",
    "        columns_MEAN = [item for sublist in columns_MEAN for item in sublist]\n",
    "        columns_MODE = [item for sublist in columns_MODE for item in sublist]\n",
    "        \n",
    "        # dataframe for indexes\n",
    "\n",
    "        df_EXTRA_CALC_MEAN = list_of_goodies[columns_EXTRA_CALC_MEAN]\n",
    "        df_EXTRA_CALC_MEAN.loc[:, ['ari_ix_sav']] /= 10 # aridity index is the value between 0 and 100. In current version of HydroATLAS (v 1.0) it's vary between 0 and 1000\n",
    "        \n",
    "        df_EXTRA_CALC_MEAN.loc[:, ['cmi_ix_s{}'.format(i) for i in monthes]] /= 100 # aridity index is the value between -1 and 1. In current version of HydroATLAS (v 1.0) it's vary between -100 and 100\n",
    "\n",
    "        df_EXTRA_CALC_MEAN = df_EXTRA_CALC_MEAN.mean()\n",
    "        \n",
    "        # dataframe for area values\n",
    "\n",
    "        df_MEAN = list_of_goodies[columns_MEAN]\n",
    "        df_MEAN.loc[:, ['tmp_dc_s{}'.format(i) for i in monthes]] /= 10 # in some regions on North-West Russia average value for Jan -83. I assume it's need to be divide by 10\n",
    "        df_MEAN = df_MEAN.mean()\n",
    "        \n",
    "\n",
    "        #dataframe for classes\n",
    "\n",
    "        df_MODE = list_of_goodies[columns_MODE]\n",
    "        df_MODE = df_MODE.replace(-9999, np.NaN) # Это вопрос: может стоит оставить \"отсутствующий класс\" \"мокрых земель\"\n",
    "        df_MODE = df_MODE.mode()\n",
    "        \n",
    "        \n",
    "        list_of_frames = [df_EXTRA_CALC_MEAN, df_MEAN, df_MODE]\n",
    "        \n",
    "        for i in range(len(list_of_frames)):\n",
    "            if type(list_of_frames[i]) == pd.Series:\n",
    "                list_of_frames[i] = list_of_frames[i].to_frame().T\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        def split_by_categories(df_ecm, df_me, df_mo):\n",
    "    \n",
    "            # basic numbers for different variables\n",
    "            monthes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "            land_cover_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22']\n",
    "            natural_vegetation = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
    "            wetland_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09']\n",
    "\n",
    "            hydrology_variables = [item for sublist in [['inu_pc_ult'], ['lka_pc_use'], ['lkv_mc_usu'],\n",
    "                                                    ['rev_mc_usu'], ['dor_pc_pva'], ['gwt_cm_sav']]\n",
    "                            for item in sublist]\n",
    "\n",
    "            physiography_variables = [item for sublist in [['ele_mt_sav'], ['slp_dg_sav'], ['sgr_dk_sav']] \n",
    "                                    for item in sublist]\n",
    "\n",
    "            climate_variables = [item for sublist in [['clz_cl_smj'], ['cls_cl_smj'], ['tmp_dc_s{}'.format(i) for i in monthes], \n",
    "                                                    ['pre_mm_s{}'.format(i) for i in monthes], ['pet_mm_s{}'.format(i) for i in monthes],\n",
    "                                                    ['aet_mm_s{}'.format(i) for i in monthes], ['ari_ix_sav'],\n",
    "                                                    ['cmi_ix_s{}'.format(i) for i in monthes], ['snw_pc_s{}'.format(i) for i in monthes]] \n",
    "                                for item in sublist]\n",
    "\n",
    "            landcover_variables = [item for sublist in [['glc_cl_smj'], ['glc_pc_s{}'.format(i) for i in land_cover_classes], \n",
    "                                                        ['pnv_cl_smj'], ['wet_cl_smj'], ['wet_pc_s{}'.format(i) for i in wetland_classes],\n",
    "                                                        ['for_pc_sse'], ['crp_pc_sse'], ['pst_pc_sse'], \n",
    "                                                        ['ire_pc_sse'], ['gla_pc_sse'], ['prm_pc_sse'], \n",
    "                                                        ['tbi_cl_smj'], ['tec_cl_smj']]\n",
    "                                for item in sublist]\n",
    "\n",
    "            soil_and_geo_variables = [item for sublist in [['cly_pc_sav'], ['slt_pc_sav'], ['snd_pc_sav'], \n",
    "                                                        ['soc_th_sav'], ['swc_pc_syr'], ['swc_pc_s{}'.format(i) for i in monthes],\n",
    "                                                        ['lit_cl_smj'], ['kar_pc_sse'], ['ero_kh_sav']]\n",
    "                                    for item in sublist]\n",
    "\n",
    "            urban_variables = [item for sublist in [['urb_pc_sse'], ['hft_ix_s93'], ['hft_ix_s09']] for item in sublist]\n",
    "\n",
    "            # dataframe of hydrology variables\n",
    "            df_HYDRO = pd.concat([\n",
    "                                    df_ecm[\n",
    "                                    df_ecm.columns[\n",
    "                                    [True if i in hydrology_variables else False for i in df_ecm.columns]\n",
    "                                                            ]],\n",
    "                                    df_me[\n",
    "                                    df_me.columns[\n",
    "                                    [True if i in hydrology_variables else False for i in df_me.columns]\n",
    "                                                            ]],\n",
    "                                    df_mo[\n",
    "                                    df_mo.columns[\n",
    "                                    [True if i in hydrology_variables else False for i in df_mo.columns]\n",
    "                                                            ]]\n",
    "                                    ], axis = 1)\n",
    "            # dataframe of physiography variables\n",
    "            df_PHYSIO = pd.concat([\n",
    "                                    df_ecm[\n",
    "                                    df_ecm.columns[\n",
    "                                    [True if i in physiography_variables else False for i in df_ecm.columns]\n",
    "                                                            ]],\n",
    "                                    df_me[\n",
    "                                    df_me.columns[\n",
    "                                    [True if i in physiography_variables else False for i in df_me.columns]\n",
    "                                                            ]],\n",
    "                                    df_mo[\n",
    "                                    df_mo.columns[\n",
    "                                    [True if i in physiography_variables else False for i in df_mo.columns]\n",
    "                                                            ]]\n",
    "                                    ], axis = 1)\n",
    "\n",
    "            # dataframe of climate variables\n",
    "            df_CLIMATE = pd.concat([\n",
    "                                    df_ecm[\n",
    "                                    df_ecm.columns[\n",
    "                                    [True if i in climate_variables else False for i in df_ecm.columns]\n",
    "                                                            ]],\n",
    "                                    df_me[\n",
    "                                    df_me.columns[\n",
    "                                    [True if i in climate_variables else False for i in df_me.columns]\n",
    "                                                            ]],\n",
    "                                    df_mo[\n",
    "                                    df_mo.columns[\n",
    "                                    [True if i in climate_variables else False for i in df_mo.columns]\n",
    "                                                            ]]\n",
    "                                    ], axis = 1)\n",
    "            # dataframe of physiography variables                       \n",
    "            df_LANDCOVER = pd.concat([\n",
    "                                    df_ecm[\n",
    "                                    df_ecm.columns[\n",
    "                                    [True if i in landcover_variables else False for i in df_ecm.columns]\n",
    "                                                            ]],\n",
    "                                    df_me[\n",
    "                                    df_me.columns[\n",
    "                                    [True if i in landcover_variables else False for i in df_me.columns]\n",
    "                                                            ]],\n",
    "                                    df_mo[\n",
    "                                    df_mo.columns[\n",
    "                                    [True if i in landcover_variables else False for i in df_mo.columns]\n",
    "                                                            ]]\n",
    "                                    ], axis = 1)\n",
    "            # dataframe of soil and geology variables\n",
    "            df_SOIL_GEO = pd.concat([\n",
    "                                    df_ecm[\n",
    "                                    df_ecm.columns[\n",
    "                                    [True if i in soil_and_geo_variables else False for i in df_ecm.columns]\n",
    "                                                            ]],\n",
    "                                    df_me[\n",
    "                                    df_me.columns[\n",
    "                                    [True if i in soil_and_geo_variables else False for i in df_me.columns]\n",
    "                                                            ]],\n",
    "                                    df_mo[\n",
    "                                    df_mo.columns[\n",
    "                                    [True if i in soil_and_geo_variables else False for i in df_mo.columns]\n",
    "                                                            ]]\n",
    "                                    ], axis = 1)\n",
    "            # dataframe of urban variables\n",
    "            df_URBAN = pd.concat([\n",
    "                                    df_ecm[\n",
    "                                    df_ecm.columns[\n",
    "                                    [True if i in urban_variables else False for i in df_ecm.columns]\n",
    "                                                            ]],\n",
    "                                    df_me[\n",
    "                                    df_me.columns[\n",
    "                                    [True if i in urban_variables else False for i in df_me.columns]\n",
    "                                                            ]],\n",
    "                                    df_mo[\n",
    "                                    df_mo.columns[\n",
    "                                    [True if i in urban_variables else False for i in df_mo.columns]\n",
    "                                                            ]]\n",
    "                                    ], axis = 1)\n",
    "            return [df_HYDRO, df_PHYSIO, df_CLIMATE, df_LANDCOVER, df_SOIL_GEO, df_URBAN]\n",
    "        \n",
    "        fin = split_by_categories(df_ecm = list_of_frames[0], df_me = list_of_frames[1], df_mo = list_of_frames[2])\n",
    "    \n",
    "    else:\n",
    "        list_of_goodies = np.NaN\n",
    "        union_geometry = np.NaN\n",
    "        fin = np.NaN \n",
    "    \n",
    "        \n",
    "    return fin, union_geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mental-anatomy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gauge_name</th>\n",
       "      <th>ID</th>\n",
       "      <th>AREA</th>\n",
       "      <th>geometry</th>\n",
       "      <th>Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hrevica - Ivanovskoe</td>\n",
       "      <td>72617.0</td>\n",
       "      <td>311</td>\n",
       "      <td>POLYGON ((29.12397 59.54101, 29.12505 59.54099...</td>\n",
       "      <td>310.172137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Izhora - Annolovo</td>\n",
       "      <td>72729.0</td>\n",
       "      <td>875</td>\n",
       "      <td>POLYGON ((30.20356 59.71164, 30.20897 59.71151...</td>\n",
       "      <td>872.109881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kovashi - Lendovshina</td>\n",
       "      <td>72552.0</td>\n",
       "      <td>722</td>\n",
       "      <td>POLYGON ((29.41333 59.92602, 29.41551 59.92598...</td>\n",
       "      <td>719.851796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lemovzha - Hotnezha</td>\n",
       "      <td>72605.0</td>\n",
       "      <td>954</td>\n",
       "      <td>POLYGON ((29.41373 59.57100, 29.41696 59.57094...</td>\n",
       "      <td>951.271639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oredezh - Bolshoe Zarechie</td>\n",
       "      <td>72584.0</td>\n",
       "      <td>282</td>\n",
       "      <td>POLYGON ((29.58405 59.55626, 29.58620 59.55621...</td>\n",
       "      <td>280.782670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Oredezh - Chikino</td>\n",
       "      <td>72585.0</td>\n",
       "      <td>325</td>\n",
       "      <td>POLYGON ((29.58409 59.55680, 29.58516 59.55678...</td>\n",
       "      <td>323.762296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Oredezh - Virica</td>\n",
       "      <td>72588.0</td>\n",
       "      <td>828</td>\n",
       "      <td>POLYGON ((29.58405 59.55626, 29.58620 59.55621...</td>\n",
       "      <td>825.279453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Orlinka - Orlinka</td>\n",
       "      <td>72592.0</td>\n",
       "      <td>211</td>\n",
       "      <td>POLYGON ((30.13084 59.33856, 30.13726 59.33841...</td>\n",
       "      <td>210.345804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sista - Srednee Raykovo</td>\n",
       "      <td>72559.0</td>\n",
       "      <td>589</td>\n",
       "      <td>POLYGON ((28.87325 59.75869, 28.88083 59.75858...</td>\n",
       "      <td>587.450235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Strelka - Oliki</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>POLYGON ((29.94785 59.77180, 29.95109 59.77173...</td>\n",
       "      <td>39.230482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Vruda - Izvoz</td>\n",
       "      <td>72610.0</td>\n",
       "      <td>329</td>\n",
       "      <td>POLYGON ((29.23738 59.54568, 29.24169 59.54561...</td>\n",
       "      <td>327.939100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Yashera - Dolgovka</td>\n",
       "      <td>72603.0</td>\n",
       "      <td>775</td>\n",
       "      <td>POLYGON ((29.76913 59.32925, 29.77020 59.32922...</td>\n",
       "      <td>772.700839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    gauge_name       ID  AREA  \\\n",
       "0         Hrevica - Ivanovskoe  72617.0   311   \n",
       "1            Izhora - Annolovo  72729.0   875   \n",
       "2        Kovashi - Lendovshina  72552.0   722   \n",
       "3          Lemovzha - Hotnezha  72605.0   954   \n",
       "4   Oredezh - Bolshoe Zarechie  72584.0   282   \n",
       "5            Oredezh - Chikino  72585.0   325   \n",
       "6             Oredezh - Virica  72588.0   828   \n",
       "7            Orlinka - Orlinka  72592.0   211   \n",
       "8      Sista - Srednee Raykovo  72559.0   589   \n",
       "9              Strelka - Oliki      NaN    39   \n",
       "10               Vruda - Izvoz  72610.0   329   \n",
       "11          Yashera - Dolgovka  72603.0   775   \n",
       "\n",
       "                                             geometry        Area  \n",
       "0   POLYGON ((29.12397 59.54101, 29.12505 59.54099...  310.172137  \n",
       "1   POLYGON ((30.20356 59.71164, 30.20897 59.71151...  872.109881  \n",
       "2   POLYGON ((29.41333 59.92602, 29.41551 59.92598...  719.851796  \n",
       "3   POLYGON ((29.41373 59.57100, 29.41696 59.57094...  951.271639  \n",
       "4   POLYGON ((29.58405 59.55626, 29.58620 59.55621...  280.782670  \n",
       "5   POLYGON ((29.58409 59.55680, 29.58516 59.55678...  323.762296  \n",
       "6   POLYGON ((29.58405 59.55626, 29.58620 59.55621...  825.279453  \n",
       "7   POLYGON ((30.13084 59.33856, 30.13726 59.33841...  210.345804  \n",
       "8   POLYGON ((28.87325 59.75869, 28.88083 59.75858...  587.450235  \n",
       "9   POLYGON ((29.94785 59.77180, 29.95109 59.77173...   39.230482  \n",
       "10  POLYGON ((29.23738 59.54568, 29.24169 59.54561...  327.939100  \n",
       "11  POLYGON ((29.76913 59.32925, 29.77020 59.32922...  772.700839  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geometry_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "permanent-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_HydroATLAS_multiple_WS(WS_geometry, path_to_HydroATLAS, path_to_results):\n",
    "    \"\"\"\n",
    "    Purpose of this function is to obtain characteristics for multiple WS\n",
    "    geometry of which are stored in GeoDataFrame with next rows:\n",
    "    ID, geometry\n",
    "    \n",
    "    WS_geometry - GeoDataFrame with geometry\n",
    "    path_to_HydroATLAS - path to .gdb file\n",
    "    path_to_results - folder where results will be saved\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    my_headache = list()\n",
    "    \n",
    "    for row in range(len(WS_geometry)):\n",
    "        my_headache.append(get_HydroATLAS_for_WS(WS = WS_geometry,\n",
    "                            WS_index = row, \n",
    "                            path_to_HydroATLAS = path_to_HydroATLAS + 'BasinATLAS_v10.gdb'))\n",
    "        print('geographic characteristics for {} calculated!'.format(WS_geometry.ID[row]))\n",
    "    \n",
    "    bool_array = list()\n",
    "    # check if value exist. To not mess with ID assignments\n",
    "    for i in range(len(my_headache)):\n",
    "        if type(my_headache[i][0]) == float:\n",
    "            bool_array.append(False)\n",
    "        else:\n",
    "            bool_array.append(True)\n",
    "            \n",
    "    # common procedure: for each WS ID - certain geographic characteristics\n",
    "    VALID_ID = [ID for i, ID in enumerate(WS_geometry.ID) if bool_array[i]]\n",
    "\n",
    "    VALID_HYDRO = [hydro[0][0] for i, hydro in enumerate(my_headache) if bool_array[i]]\n",
    "    hydro_df = pd.concat(VALID_HYDRO).dropna().reset_index(drop = True)\n",
    "    hydro_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "    hydro_df.to_csv(path_to_results + 'hydro.csv', index = False)\n",
    "\n",
    "    VALID_PHYSIO = [physio[0][1] for i, physio in enumerate(my_headache) if bool_array[i]]\n",
    "    physio_df = pd.concat(VALID_PHYSIO).dropna().reset_index(drop = True)\n",
    "    physio_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "    physio_df.to_csv(path_to_results + 'physio.csv', index = False)\n",
    "\n",
    "    VALID_CLIMATE = [climate[0][2] for i, climate in enumerate(my_headache) if bool_array[i]]\n",
    "    climate_df = pd.concat(VALID_CLIMATE).dropna().reset_index(drop = True)\n",
    "    climate_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "    climate_df.to_csv(path_to_results + 'climate.csv', index = False)\n",
    "\n",
    "    VALID_URBAN = [urban[0][5] for i, urban in enumerate(my_headache) if bool_array[i]]\n",
    "    urban_df = pd.concat(VALID_URBAN).dropna().reset_index(drop = True)\n",
    "    urban_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "    urban_df.to_csv(path_to_results + 'urban.csv', index = False)\n",
    "\n",
    "    VALID_LANDCOVER = [landcover[0][3] for i, landcover in enumerate(my_headache) if bool_array[i]]\n",
    "    landcover_df = pd.concat(VALID_LANDCOVER).dropna(thresh = 4).reset_index(drop = True)\n",
    "    landcover_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "    landcover_df.to_csv(path_to_results + 'landcover.csv', index = False)\n",
    "\n",
    "    VALID_SOIL_GEO = [soil_geo[0][4] for i, soil_geo in enumerate(my_headache) if bool_array[i]]\n",
    "    soil_geo_df = pd.concat(VALID_SOIL_GEO).dropna().reset_index(drop = True)\n",
    "    soil_geo_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "    soil_geo_df.to_csv(path_to_results + 'soil_geo.csv', index = False)\n",
    "\n",
    "    VALID_GEOM_HydroATLAS = [geometry[1] for i, geometry in enumerate(my_headache) if bool_array[i]]\n",
    "    geometry_df = gpd.GeoDataFrame(VALID_GEOM_HydroATLAS,\n",
    "                                   columns={'geometry'}).reset_index(drop = True)\n",
    "    geometry_df.insert(loc = 0, column = 'ID', value = VALID_ID)\n",
    "\n",
    "    geometry_df.to_csv(path_to_results + 'geometry_HydroATLAS_subB.csv', index = False)\n",
    "    \n",
    "    return 'Check results folder !'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-tiffany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geographic characteristics for 72617.0 calculated!\n",
      "geographic characteristics for 72729.0 calculated!\n"
     ]
    }
   ],
   "source": [
    "calculate_HydroATLAS_multiple_WS(WS_geometry=geometry_data,\n",
    "                                path_to_HydroATLAS=HydroATLAS_data_path,\n",
    "                                path_to_results=path_to_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-preference",
   "metadata": {},
   "source": [
    "### Abbreviation explanation (before the slash is the category where this variable are related\n",
    "##### inu_pc - Hydrology / Inundation Extent\n",
    "##### lka_pc - Hydrology / Limnicity (% per area)\n",
    "##### lkv_mc - Hydrology / Lake volume\n",
    "##### rev_mc - Hydrology / Reservoir volume\n",
    "##### dor_pc - Hydrology / Degree of Regulation\n",
    "##### gwt_cm - Hydrology / Groundwater table depth\n",
    "##### ele_mt - Physiography / Elevation\n",
    "##### slp_dg - Physiography / Terrain slope\n",
    "##### sgr_dk - Physiography / Stream gradient\n",
    "##### clz_cl - Climate / Climate zones\n",
    "##### cls_cl - Climate / Climate strata\n",
    "##### tmp_dc - Climate / Air Temperature\n",
    "##### pre_mm - Climate / Precipitation\n",
    "##### pet_mm - Climate / Potential evapotraspiration\n",
    "##### aet_mm - Climate / Actual evapotranspiration\n",
    "##### ari_ix - Climate / Global aridity index\n",
    "##### cmi_ix - Climate / Climate moisture index\n",
    "##### snw_pc - Climate / Snow cover extent\n",
    "##### glc_cl - Landcover / Land cover classes\n",
    "##### glc_pc - Landcover / Land cover extent\n",
    "##### pnv_cl - Landcover / Potential natural vegetation classes\n",
    "##### wet_cl - Landcover / Wetland classes\n",
    "##### wet_pc - Landcover / Wetland extent\n",
    "##### for_pc - Landcover / Forest cover extent\n",
    "##### crp_pc - Landcover / Cropland extent\n",
    "##### pst_pc - Landcover / Pasture extent\n",
    "##### ire_pc - Landcover / Irrigated area extent\n",
    "##### gla_pc - Landcover / Glacier extent\n",
    "##### prm_pc - Landcover / Permafrost extent\n",
    "##### tbi_cl - Landcover / Terrestrial biomes\n",
    "##### tec_cl - Landcover / Terresterial ecoregions \n",
    "##### cly_pc - Soils & Geology / Clay fraction in soil\n",
    "##### slt_pc - Soils & Geology / Silt fraction in soil\n",
    "##### snd_pc - Soils & Geology / Sand fraction in soil\n",
    "##### soc_th - Soils & Geology / Organic carbon content in soil\n",
    "##### swc_pc - Soils & Geology / Soil water content\n",
    "##### lit_cl - Soils & Geology / Lithological classes\n",
    "##### kar_pc - Soils & Geology / Karst area extent\n",
    "##### ero_kh - Soils & Geology / Soil erosion\n",
    "##### urb_pc - Anthropogenic / Urban extent\n",
    "##### hft_ix_93 - Anthropogenic / Human footprint (year 1993)\n",
    "##### hft_ix_09 - Anthropogenic / Human footprint (year 2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-advice",
   "metadata": {},
   "source": [
    "### Dimension explanation\n",
    "##### pc- percentage of something\n",
    "##### mc - million cubic meters\n",
    "##### cm - centimeters\n",
    "##### mt - meters or meters above sea level (m.a.s.l)\n",
    "##### dk - decimeter per kilometer\n",
    "##### cl - classes\n",
    "##### dg - degrees\n",
    "##### dc - degreec celsius °\n",
    "##### mm - millimeters\n",
    "##### ix - index value\n",
    "##### kh - kilogram per hectare (kg/ha) per year\n",
    "##### th - tonnes per hectare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-selection",
   "metadata": {},
   "source": [
    "### Prefix explanation\n",
    "##### s - in sub-basin\n",
    "##### p - at sub-basin pour point or at reach pour point\n",
    "##### u - in total watershed\n",
    "##### c - in reach catchment (i.e. the local catchment that drains directly into the reach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-applicant",
   "metadata": {},
   "source": [
    "### Statistical aggregation index\n",
    "##### mj - spatial majority\n",
    "##### se - spatial extent(%)\n",
    "##### yr - annual average\n",
    "##### s{01..12} - montly average\n",
    "##### lt - long-term maximum\n",
    "##### su - sum\n",
    "##### va - value\n",
    "##### av - average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-weather",
   "metadata": {},
   "source": [
    "#### Transition to name from number in _cl variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cultural-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx = pd.ExcelFile('/mnt/c/education/HSI/aspirantura/CAMELS_ru/files/HydroAtlas/Data/HydroATLAS_v10_Legends.xlsx')\n",
    "my_cl_names = [i for i in xlsx.sheet_names if i in ['clz_cl', 'cls_cl', 'glc_cl', 'pnv_cl', 'wet_cl', 'tbi_cl', 'tec_cl', 'lit_cl']]\n",
    "name_schemes = [pd.read_excel('/mnt/c/education/HSI/aspirantura/CAMELS_ru/files/HydroAtlas/Data/HydroATLAS_v10_Legends.xlsx', \n",
    "                              sheet_name = i) for i in my_cl_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "finite-respondent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEnZ_ID</th>\n",
       "      <th>GEnZ_Name</th>\n",
       "      <th>GEnZ_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Arctic 1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Arctic 2</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Extremely cold and wet 1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Extremely cold and wet 2</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Cold and wet</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Extremely cold and mesic</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Cold and mesic</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Cool temperate and dry</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Cool temperate and xeric</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Cool temperate and moist</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Warm temperate and mesic</td>\n",
       "      <td>K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Warm temperate and xeric</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Hot and mesic</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Hot and dry</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Hot and arid</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Extremely hot and arid</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Extremely hot and xeric</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Extremely hot and moist</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    GEnZ_ID                 GEnZ_Name GEnZ_Code\n",
       "0         1                  Arctic 1         A\n",
       "1         2                  Arctic 2         B\n",
       "2         3  Extremely cold and wet 1         C\n",
       "3         4  Extremely cold and wet 2         D\n",
       "4         5              Cold and wet         E\n",
       "5         6  Extremely cold and mesic         F\n",
       "6         7            Cold and mesic         G\n",
       "7         8    Cool temperate and dry         H\n",
       "8         9  Cool temperate and xeric         I\n",
       "9        10  Cool temperate and moist         J\n",
       "10       11  Warm temperate and mesic         K\n",
       "11       12  Warm temperate and xeric         L\n",
       "12       13             Hot and mesic         M\n",
       "13       14               Hot and dry         N\n",
       "14       15              Hot and arid         O\n",
       "15       16    Extremely hot and arid         P\n",
       "16       17   Extremely hot and xeric         Q\n",
       "17       18   Extremely hot and moist         R"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_schemes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-skill",
   "metadata": {},
   "source": [
    "### Read Open Forecast shape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pregnant-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_shape = gpd.read_file(\n",
    "    '/mnt/c/education/HSI/aspirantura/CAMELS_ru/files/openf_gauges_watersheds/watersheds_openf.shp'\n",
    ")\n",
    "big_shape = big_shape[['code', 'name_en', 'geometry']]\n",
    "big_shape = big_shape.rename(columns={\"code\": \"ID\"})\n",
    "big_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-filling",
   "metadata": {},
   "source": [
    "\n",
    "    [ - LIST OF HydroATLAS vars Which are List of DF for Each characteristic\n",
    "         [\n",
    "             - different characteristics in strict order\n",
    "         ]\n",
    "     ] - len of this equal to big_shape.ID as well as order\n",
    "\n",
    "    EZ SAVE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-blake",
   "metadata": {},
   "source": [
    "### Проверка осреднений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "grateful-folks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmp_dc_s01</th>\n",
       "      <th>tmp_dc_s02</th>\n",
       "      <th>tmp_dc_s03</th>\n",
       "      <th>tmp_dc_s04</th>\n",
       "      <th>tmp_dc_s05</th>\n",
       "      <th>tmp_dc_s06</th>\n",
       "      <th>tmp_dc_s07</th>\n",
       "      <th>tmp_dc_s08</th>\n",
       "      <th>tmp_dc_s09</th>\n",
       "      <th>tmp_dc_s10</th>\n",
       "      <th>tmp_dc_s11</th>\n",
       "      <th>tmp_dc_s12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-80.666667</td>\n",
       "      <td>-76.666667</td>\n",
       "      <td>-33.666667</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>103.666667</td>\n",
       "      <td>152.666667</td>\n",
       "      <td>170.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>103.666667</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-8.666667</td>\n",
       "      <td>-52.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tmp_dc_s01 tmp_dc_s02 tmp_dc_s03 tmp_dc_s04  tmp_dc_s05  tmp_dc_s06  \\\n",
       "0 -80.666667 -76.666667 -33.666667  36.666667  103.666667  152.666667   \n",
       "\n",
       "  tmp_dc_s07 tmp_dc_s08  tmp_dc_s09 tmp_dc_s10 tmp_dc_s11 tmp_dc_s12  \n",
       "0      170.0      154.0  103.666667       48.0  -8.666667 -52.666667  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_MEAN.loc[:, ['tmp_dc_s{}'.format(i) for i in monthes]] # tmp / 10 ✓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "spectacular-genre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_mm_s01</th>\n",
       "      <th>pre_mm_s02</th>\n",
       "      <th>pre_mm_s03</th>\n",
       "      <th>pre_mm_s04</th>\n",
       "      <th>pre_mm_s05</th>\n",
       "      <th>pre_mm_s06</th>\n",
       "      <th>pre_mm_s07</th>\n",
       "      <th>pre_mm_s08</th>\n",
       "      <th>pre_mm_s09</th>\n",
       "      <th>pre_mm_s10</th>\n",
       "      <th>pre_mm_s11</th>\n",
       "      <th>pre_mm_s12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>30.666667</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>38.666667</td>\n",
       "      <td>44.666667</td>\n",
       "      <td>70.333333</td>\n",
       "      <td>79.333333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>77.333333</td>\n",
       "      <td>69.0</td>\n",
       "      <td>65.333333</td>\n",
       "      <td>55.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pre_mm_s01 pre_mm_s02 pre_mm_s03 pre_mm_s04 pre_mm_s05 pre_mm_s06  \\\n",
       "0       40.0  30.666667  36.666667  38.666667  44.666667  70.333333   \n",
       "\n",
       "  pre_mm_s07 pre_mm_s08 pre_mm_s09 pre_mm_s10 pre_mm_s11 pre_mm_s12  \n",
       "0  79.333333  86.666667  77.333333       69.0  65.333333  55.666667  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_MEAN.loc[:, ['pre_mm_s{}'.format(i) for i in monthes]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "compressed-manitoba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sgr_dk_sav</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sgr_dk_sav\n",
       "0  27.666667"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_MEAN.loc[:, ['sgr_dk_sav']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "automatic-innocent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snw_pc_s01</th>\n",
       "      <th>snw_pc_s02</th>\n",
       "      <th>snw_pc_s03</th>\n",
       "      <th>snw_pc_s04</th>\n",
       "      <th>snw_pc_s05</th>\n",
       "      <th>snw_pc_s06</th>\n",
       "      <th>snw_pc_s07</th>\n",
       "      <th>snw_pc_s08</th>\n",
       "      <th>snw_pc_s09</th>\n",
       "      <th>snw_pc_s10</th>\n",
       "      <th>snw_pc_s11</th>\n",
       "      <th>snw_pc_s12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94.666667</td>\n",
       "      <td>99.333333</td>\n",
       "      <td>89.0</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>24.666667</td>\n",
       "      <td>64.333333</td>\n",
       "      <td>91.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  snw_pc_s01 snw_pc_s02 snw_pc_s03 snw_pc_s04 snw_pc_s05 snw_pc_s06  \\\n",
       "0  94.666667  99.333333       89.0  20.333333   1.333333   1.666667   \n",
       "\n",
       "  snw_pc_s07 snw_pc_s08 snw_pc_s09 snw_pc_s10 snw_pc_s11 snw_pc_s12  \n",
       "0        0.0        1.0   3.333333  24.666667  64.333333  91.666667  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_MEAN.loc[:, ['snw_pc_s{}'.format(i) for i in monthes]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "private-strategy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kar_pc_sse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  kar_pc_sse\n",
       "0        0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_MEAN.loc[:, ['kar_pc_sse']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-decimal",
   "metadata": {},
   "source": [
    "### TEST ORIGINAL CAMELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "excessive-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчёт произведён только для одного водосбора. Если на нём всё получилось - получиться и на всех\n",
    "\n",
    "camels_test_gauge = pd.read_csv('/mnt/c/education/HSI/aspirantura/CAMELS_ru/files/basin_dataset_public_v1p2/usgs_streamflow/01/01013500_streamflow_qc.txt', \n",
    "                               names = ['ID', 'year', 'month', 'day', 'discharge', 'idk'], delim_whitespace=True)\n",
    "camels_test_gauge['date'] = pd.to_datetime(camels_test_gauge[['year', 'month', 'day']])\n",
    "camels_test_gauge = camels_test_gauge.drop(['year', 'month', 'day'], axis = 1)\n",
    "camels_test_gauge['discharge'] = camels_test_gauge['discharge'].replace(-999, np.NaN)\n",
    "camels_test_gauge['discharge'] *= 0.0283168\n",
    "\n",
    "camels_test_gauge = camels_test_gauge.set_index('date', drop = False)['1989-10-01' :'2009-09-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "furnished-china",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HUC_02  GAGE_ID</th>\n",
       "      <th>GAGE_NAME</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LONG</th>\n",
       "      <th>DRAINAGE AREA (KM^2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1013500</td>\n",
       "      <td>Fish River near Fort Kent, M...</td>\n",
       "      <td>47.23739</td>\n",
       "      <td>-68.58264</td>\n",
       "      <td>2252.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1022500</td>\n",
       "      <td>Narraguagus River at Cherryfield, M...</td>\n",
       "      <td>44.60797</td>\n",
       "      <td>-67.93524</td>\n",
       "      <td>573.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HUC_02  GAGE_ID                                          GAGE_NAME  \\\n",
       "0          1013500                    Fish River near Fort Kent, M...   \n",
       "1          1022500             Narraguagus River at Cherryfield, M...   \n",
       "\n",
       "        LAT      LONG  DRAINAGE AREA (KM^2)  \n",
       "0  47.23739 -68.58264                2252.7  \n",
       "1  44.60797 -67.93524                 573.6  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_AREA_for_test_gauge = pd.read_csv('/mnt/c/education/HSI/aspirantura/CAMELS_ru/files/basin_dataset_public_v1p2/basin_metadata/gauge_information.txt', \n",
    "                                         sep = \"\\t+\", \n",
    "                                         nrows = 2, \n",
    "                                         engine = 'python').reset_index(drop = True)\n",
    "camels_AREA_for_test_gauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "continuing-crystal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>discharge</th>\n",
       "      <th>idk</th>\n",
       "      <th>date</th>\n",
       "      <th>layer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1989-10-01</th>\n",
       "      <td>1013500</td>\n",
       "      <td>8.495040</td>\n",
       "      <td>A</td>\n",
       "      <td>1989-10-01</td>\n",
       "      <td>0.325819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-10-02</th>\n",
       "      <td>1013500</td>\n",
       "      <td>8.410090</td>\n",
       "      <td>A</td>\n",
       "      <td>1989-10-02</td>\n",
       "      <td>0.322560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-10-03</th>\n",
       "      <td>1013500</td>\n",
       "      <td>8.863158</td>\n",
       "      <td>A</td>\n",
       "      <td>1989-10-03</td>\n",
       "      <td>0.339937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-10-04</th>\n",
       "      <td>1013500</td>\n",
       "      <td>8.381773</td>\n",
       "      <td>A</td>\n",
       "      <td>1989-10-04</td>\n",
       "      <td>0.321474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989-10-05</th>\n",
       "      <td>1013500</td>\n",
       "      <td>8.098605</td>\n",
       "      <td>A</td>\n",
       "      <td>1989-10-05</td>\n",
       "      <td>0.310614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-26</th>\n",
       "      <td>1013500</td>\n",
       "      <td>8.041971</td>\n",
       "      <td>A</td>\n",
       "      <td>2009-09-26</td>\n",
       "      <td>0.308442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-27</th>\n",
       "      <td>1013500</td>\n",
       "      <td>8.013654</td>\n",
       "      <td>A</td>\n",
       "      <td>2009-09-27</td>\n",
       "      <td>0.307356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-28</th>\n",
       "      <td>1013500</td>\n",
       "      <td>8.749891</td>\n",
       "      <td>A</td>\n",
       "      <td>2009-09-28</td>\n",
       "      <td>0.335593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-29</th>\n",
       "      <td>1013500</td>\n",
       "      <td>10.392266</td>\n",
       "      <td>A</td>\n",
       "      <td>2009-09-29</td>\n",
       "      <td>0.398585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-30</th>\n",
       "      <td>1013500</td>\n",
       "      <td>11.609888</td>\n",
       "      <td>A</td>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>0.445285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7305 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  discharge idk       date     layer\n",
       "date                                                   \n",
       "1989-10-01  1013500   8.495040   A 1989-10-01  0.325819\n",
       "1989-10-02  1013500   8.410090   A 1989-10-02  0.322560\n",
       "1989-10-03  1013500   8.863158   A 1989-10-03  0.339937\n",
       "1989-10-04  1013500   8.381773   A 1989-10-04  0.321474\n",
       "1989-10-05  1013500   8.098605   A 1989-10-05  0.310614\n",
       "...             ...        ...  ..        ...       ...\n",
       "2009-09-26  1013500   8.041971   A 2009-09-26  0.308442\n",
       "2009-09-27  1013500   8.013654   A 2009-09-27  0.307356\n",
       "2009-09-28  1013500   8.749891   A 2009-09-28  0.335593\n",
       "2009-09-29  1013500  10.392266   A 2009-09-29  0.398585\n",
       "2009-09-30  1013500  11.609888   A 2009-09-30  0.445285\n",
       "\n",
       "[7305 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_test_gauge['layer'] = (86400 * camels_test_gauge['discharge'] * 10**9) / (camels_AREA_for_test_gauge['DRAINAGE AREA (KM^2)'][0] * 10**12)\n",
    "camels_test_gauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "seasonal-tyler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean layer of discharge calculation\n",
      "Flow duration curve calculation\n",
      "Base flow index calculation\n",
      "Расчёт на 0 посту закончилась за 7 секунд\n",
      "Base Flow Index splitting\n",
      "5% quantile calculation\n",
      "95% quantile calculation\n",
      "High discharge frequency calculation\n",
      "High discharge duration calculation\n",
      "High discharge frequency calculation\n",
      "Low discharge duration calculation\n",
      "Zero discharge frequency\n",
      "Half flow date calculation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gauge_id</th>\n",
       "      <th>q_mean</th>\n",
       "      <th>slope_fdc</th>\n",
       "      <th>baseflow_index</th>\n",
       "      <th>q5</th>\n",
       "      <th>q95</th>\n",
       "      <th>high_q_freq</th>\n",
       "      <th>high_q_dur</th>\n",
       "      <th>low_q_freq</th>\n",
       "      <th>low_q_dur</th>\n",
       "      <th>zero_q_freq</th>\n",
       "      <th>hfd_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1013500</td>\n",
       "      <td>1.6992</td>\n",
       "      <td>2.6965</td>\n",
       "      <td>0.5357</td>\n",
       "      <td>0.2411</td>\n",
       "      <td>6.3730</td>\n",
       "      <td>6.1000</td>\n",
       "      <td>8.7143</td>\n",
       "      <td>41.3500</td>\n",
       "      <td>20.1707</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>207.2500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gauge_id  q_mean slope_fdc baseflow_index      q5     q95 high_q_freq  \\\n",
       "0   1013500  1.6992    2.6965         0.5357  0.2411  6.3730      6.1000   \n",
       "\n",
       "  high_q_dur low_q_freq low_q_dur zero_q_freq  hfd_mean  \n",
       "0     8.7143    41.3500   20.1707      0.0000  207.2500  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydro_signatures_for_CAMELS(hydro_data = [camels_test_gauge], path_to_results = path_to_results + 'test',\n",
    "                            ID = [camels_test_gauge['ID'][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "intimate-telescope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gauge_id</th>\n",
       "      <th>q_mean</th>\n",
       "      <th>runoff_ratio</th>\n",
       "      <th>slope_fdc</th>\n",
       "      <th>baseflow_index</th>\n",
       "      <th>stream_elas</th>\n",
       "      <th>q5</th>\n",
       "      <th>q95</th>\n",
       "      <th>high_q_freq</th>\n",
       "      <th>high_q_dur</th>\n",
       "      <th>low_q_freq</th>\n",
       "      <th>low_q_dur</th>\n",
       "      <th>zero_q_freq</th>\n",
       "      <th>hfd_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1013500.0</td>\n",
       "      <td>1.699155</td>\n",
       "      <td>0.543437</td>\n",
       "      <td>1.528219</td>\n",
       "      <td>0.585226</td>\n",
       "      <td>1.845324</td>\n",
       "      <td>0.241106</td>\n",
       "      <td>6.373021</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.714286</td>\n",
       "      <td>41.35</td>\n",
       "      <td>20.170732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gauge_id    q_mean  runoff_ratio  slope_fdc  baseflow_index  stream_elas  \\\n",
       "0  1013500.0  1.699155      0.543437   1.528219        0.585226     1.845324   \n",
       "\n",
       "         q5       q95  high_q_freq  high_q_dur  low_q_freq  low_q_dur  \\\n",
       "0  0.241106  6.373021          6.1    8.714286       41.35  20.170732   \n",
       "\n",
       "   zero_q_freq  hfd_mean  \n",
       "0          0.0    207.25  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/mnt/c/education/HSI/aspirantura/CAMELS_ru/literature/camels_attributes_v2.0/camels_hydro.txt', nrows = 2, sep = ';').iloc[0].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "important-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "colored-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = copy.deepcopy(camels_test_gauge.layer)\n",
    "test = test.reset_index(drop = True).to_frame()\n",
    "test = test.sort_values('layer', ascending = False)\n",
    "test['m'] = [i+1 for i in range(len(test))]\n",
    "test['P'] = [100 * (i/7306) for i in test['m']]\n",
    "test = test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "complimentary-shelter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>m</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.520487</td>\n",
       "      <td>2411</td>\n",
       "      <td>33.000274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.520487</td>\n",
       "      <td>2412</td>\n",
       "      <td>33.013961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.520487</td>\n",
       "      <td>2413</td>\n",
       "      <td>33.027649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.520487</td>\n",
       "      <td>2414</td>\n",
       "      <td>33.041336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.520487</td>\n",
       "      <td>2415</td>\n",
       "      <td>33.055023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>0.046701</td>\n",
       "      <td>7301</td>\n",
       "      <td>99.931563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>0.045615</td>\n",
       "      <td>7302</td>\n",
       "      <td>99.945250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>0.045615</td>\n",
       "      <td>7303</td>\n",
       "      <td>99.958938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>0.045615</td>\n",
       "      <td>7304</td>\n",
       "      <td>99.972625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>0.045615</td>\n",
       "      <td>7305</td>\n",
       "      <td>99.986313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4895 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         layer     m          P\n",
       "0     1.520487  2411  33.000274\n",
       "1     1.520487  2412  33.013961\n",
       "2     1.520487  2413  33.027649\n",
       "3     1.520487  2414  33.041336\n",
       "4     1.520487  2415  33.055023\n",
       "...        ...   ...        ...\n",
       "4890  0.046701  7301  99.931563\n",
       "4891  0.045615  7302  99.945250\n",
       "4892  0.045615  7303  99.958938\n",
       "4893  0.045615  7304  99.972625\n",
       "4894  0.045615  7305  99.986313\n",
       "\n",
       "[4895 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['P'] > 33].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "standard-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_33 = test[test['P'] > 33].reset_index(drop = True).loc[0, 'layer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "signed-staff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.520486584099081, 1.520486584099081)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_33, np.nanpercentile(camels_test_gauge.layer.dropna().to_numpy(), q = 100 - 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "absent-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_66 = test[test['P'] > 66].reset_index(drop = True).loc[0, 'layer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "quick-moisture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6244855613264083, 0.6244855613264083)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_66, np.nanpercentile(camels_test_gauge.layer.dropna().to_numpy(), q = 100 - 66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "governmental-skirt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6965378024424225"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(math.log(np.nanpercentile(camels_test_gauge.layer.dropna().to_numpy(), q = 100 - 33)) - \n",
    " math.log(np.nanpercentile(camels_test_gauge.layer.dropna().to_numpy(), q = 100 - 66))) / (0.66 - 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-algorithm",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-airline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-expression",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-depth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-decade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-average",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-secretariat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-locator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-waste",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-comedy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-wedding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-ownership",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-appliance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-chamber",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-mixture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-medicaid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-aerospace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-focus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "worthy-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_q_frequency_old(Valid_gauges_in_mm, number_of_Nan): # MY\n",
    "        \n",
    "    hydro_data = Valid_gauges_in_mm\n",
    "    \n",
    "    every_gauge_split_by_year_mm = split_by_year(hydro_data, number_of_Nan)\n",
    "\n",
    "    temp_high_q_f = [[] for _ in every_gauge_split_by_year_mm]\n",
    "\n",
    "    for i, gauge in enumerate(every_gauge_split_by_year_mm):\n",
    "        median_x9 = hydro_data[i].layer.dropna().median() * 9\n",
    "        for year in gauge:\n",
    "            if len(gauge) > 1:\n",
    "                temp_high_q_f[i].append(len(year[year.layer > median_x9])/len(year)*100)\n",
    "            else:\n",
    "                temp_high_q_f[i].append(np.NaN)\n",
    "\n",
    "    high_q_f = list()\n",
    "    for gauge in temp_high_q_f:\n",
    "        if len(gauge) > 1:\n",
    "            high_q_f.append(np.nanmean([np.NaN if i == 0 else i for i in gauge]))\n",
    "        else:\n",
    "            high_q_f.append(np.NaN)\n",
    "\n",
    "    print('High discharge frequency calculation Abramov')\n",
    "    \n",
    "    return high_q_f\n",
    "\n",
    "def low_q_frequency_old(Valid_gauges_in_mm, number_of_Nan): # MY\n",
    "        \n",
    "    hydro_data = Valid_gauges_in_mm\n",
    "    \n",
    "    every_gauge_split_by_year_mm = split_by_year(hydro_data, number_of_Nan)\n",
    "\n",
    "    temp_high_q_f = [[] for _ in every_gauge_split_by_year_mm]\n",
    "\n",
    "    for i, gauge in enumerate(every_gauge_split_by_year_mm):\n",
    "        mean_x02 = hydro_data[i].layer.dropna().mean() * 0.2\n",
    "        for year in gauge:\n",
    "            if len(gauge) > 1:\n",
    "                temp_high_q_f[i].append(len(year[year.layer < mean_x02])/len(year)*100)\n",
    "            else:\n",
    "                temp_high_q_f[i].append(np.NaN)\n",
    "\n",
    "    low_q_f = list()\n",
    "    for gauge in temp_high_q_f:\n",
    "        if len(gauge) > 1:\n",
    "            low_q_f.append(np.nanmean([np.NaN if i == 0 else i for i in gauge]))\n",
    "        else:\n",
    "            low_q_f.append(np.NaN)\n",
    "\n",
    "    print('Low discharge frequency calculation Abramov')\n",
    "    \n",
    "    return low_q_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "stable-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_q_frequency(Valid_gauges_in_mm): #Anthony Ladson version\n",
    "    \n",
    "    hydro_data = Valid_gauges_in_mm\n",
    "    \n",
    "    high_q_f = list()\n",
    "    \n",
    "    for gauge in hydro_data:\n",
    "        if len(gauge) > 1:\n",
    "            high_q_f.append(\n",
    "            len(gauge[gauge.layer.dropna() > gauge.layer.dropna().median() * 9])/len(gauge.layer.dropna()) * 365.25\n",
    "            )\n",
    "        else:\n",
    "            high_q_f.append(np.NaN)\n",
    "\n",
    "    print('High discharge frequency calculation Ladson')\n",
    "\n",
    "    return high_q_f\n",
    "\n",
    "def low_q_frequency(Valid_gauges_in_mm): #Anthony Ladson version\n",
    "    \n",
    "    hydro_data = Valid_gauges_in_mm\n",
    "    \n",
    "    low_q_f = list()\n",
    "    \n",
    "    for gauge in hydro_data:\n",
    "        if len(gauge) > 1:\n",
    "            low_q_f.append(\n",
    "            len(gauge[gauge.layer.dropna() < gauge.layer.dropna().mean() * 0.2])/len(gauge.layer.dropna()) * 365.25\n",
    "            )\n",
    "        else:\n",
    "            low_q_f.append(np.NaN)\n",
    "\n",
    "    print('High discharge frequency calculation Ladson')\n",
    "\n",
    "    return low_q_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "labeled-company",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High discharge frequency calculation Abramov\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.844444773481378]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_q_frequency_old([camels_test_gauge], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "embedded-ballet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High discharge frequency calculation Ladson\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.1]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_q_frequency([camels_test_gauge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "refined-outdoors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low discharge frequency calculation Abramov\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12.979052556333396]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_q_frequency_old([camels_test_gauge], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "funny-majority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High discharge frequency calculation Ladson\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[41.35]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_q_frequency([camels_test_gauge])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
