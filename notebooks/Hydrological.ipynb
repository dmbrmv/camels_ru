{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63529950",
   "metadata": {},
   "source": [
    "# Comprehensive Hydrological Analysis for CAMELS-RU Dataset\n",
    "\n",
    "This notebook demonstrates the usage of the reorganized hydrological statistics package for comprehensive analysis of multiple gauge stations. The new modular approach allows for efficient calculation of standardized hydrological indices across the entire CAMELS-RU dataset.\n",
    "\n",
    "## Features:\n",
    "- **Batch processing** of multiple gauge stations\n",
    "- **Comprehensive metrics** from all hydrological modules\n",
    "- **Standardized output** as DataFrame with gauge_id as index\n",
    "- **Performance optimized** calculations\n",
    "- **Error handling** for incomplete or problematic data\n",
    "\n",
    "## Modules Used:\n",
    "- `src.hydro.base_flow` - Base flow separation and BFI calculation\n",
    "- `src.hydro.flow_duration` - Flow Duration Curve analysis\n",
    "- `src.hydro.flow_extremes` - High/low flow analysis\n",
    "- `src.hydro.flow_timing` - Temporal flow characteristics\n",
    "- `src.hydro.flow_variability` - Multi-scale variability analysis\n",
    "- `src.hydro.flow_indices` - Comprehensive hydrological indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import the new modular hydro package\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.hydro import (\n",
    "    calculate_comprehensive_metrics,\n",
    "    BaseFlowSeparation,\n",
    "    FlowDurationCurve,\n",
    "    FlowExtremes,\n",
    "    FlowTiming,\n",
    "    FlowVariability,\n",
    "    HydrologicalIndices,\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8771a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load discharge data\n",
    "print(\"Loading discharge data...\")\n",
    "discharge_data = pd.read_csv(\"../discharge.csv\", index_col=\"date\")\n",
    "discharge_data.index = pd.to_datetime(discharge_data.index)\n",
    "\n",
    "# Remove columns with too many NaN values (less than 2 years of data)\n",
    "min_data_points = 730  # Minimum 2 years of daily data\n",
    "discharge_data = discharge_data.dropna(axis=1, thresh=min_data_points)\n",
    "\n",
    "print(f\"Loaded data for {len(discharge_data.columns)} gauge stations\")\n",
    "print(f\"Date range: {discharge_data.index.min()} to {discharge_data.index.max()}\")\n",
    "print(f\"Total data points per station (max): {len(discharge_data)}\")\n",
    "\n",
    "# Display basic info about the dataset\n",
    "discharge_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd63a54",
   "metadata": {},
   "source": [
    "## Core Function: Batch Hydrological Analysis\n",
    "\n",
    "The following function processes multiple gauge stations efficiently and returns a standardized DataFrame with comprehensive hydrological statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6550cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hydrological_statistics_batch(\n",
    "    discharge_data: pd.DataFrame,\n",
    "    gauge_ids: Optional[List[str]] = None,\n",
    "    min_data_years: float = 2.0,\n",
    "    include_bfi: bool = True,\n",
    "    include_detailed_metrics: bool = True,\n",
    "    progress_bar: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive hydrological statistics for multiple gauge stations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    discharge_data : pd.DataFrame\n",
    "        DataFrame with datetime index and gauge IDs as columns\n",
    "    gauge_ids : List[str], optional\n",
    "        List of specific gauge IDs to process. If None, processes all columns\n",
    "    min_data_years : float\n",
    "        Minimum years of data required for analysis (default: 2.0)\n",
    "    include_bfi : bool\n",
    "        Whether to calculate BFI (computationally intensive, default: True)\n",
    "    include_detailed_metrics : bool\n",
    "        Whether to include all detailed metrics from all modules (default: True)\n",
    "    progress_bar : bool\n",
    "        Whether to show progress bar (default: True)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with gauge_id as index and calculated statistics as columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Select gauge IDs to process\n",
    "    if gauge_ids is None:\n",
    "        gauge_ids = discharge_data.columns.tolist()\n",
    "\n",
    "    # Filter gauge IDs that actually exist in the data\n",
    "    available_gauges = [gid for gid in gauge_ids if gid in discharge_data.columns]\n",
    "\n",
    "    print(f\"Processing {len(available_gauges)} gauge stations...\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    results = {}\n",
    "    failed_gauges = []\n",
    "\n",
    "    # Setup progress bar\n",
    "    iterator = tqdm(available_gauges) if progress_bar else available_gauges\n",
    "\n",
    "    for gauge_id in iterator:\n",
    "        try:\n",
    "            if progress_bar:\n",
    "                iterator.set_description(f\"Processing {gauge_id}\")\n",
    "\n",
    "            # Extract discharge series for this gauge\n",
    "            discharge_series = discharge_data[gauge_id].dropna()\n",
    "\n",
    "            # Check minimum data requirement\n",
    "            years_of_data = len(discharge_series) / 365.25\n",
    "            if years_of_data < min_data_years:\n",
    "                if progress_bar:\n",
    "                    iterator.set_postfix({\"status\": f\"skipped - {years_of_data:.1f}y\"})\n",
    "                continue\n",
    "\n",
    "            # Calculate comprehensive metrics\n",
    "            metrics = calculate_comprehensive_metrics(\n",
    "                discharge_series, include_bfi=include_bfi, include_all_modules=include_detailed_metrics\n",
    "            )\n",
    "\n",
    "            # Add basic data quality metrics\n",
    "            metrics.update(\n",
    "                {\n",
    "                    \"data_years\": years_of_data,\n",
    "                    \"data_points\": len(discharge_series),\n",
    "                    \"data_completeness\": len(discharge_series) / len(discharge_data) * 100,\n",
    "                    \"start_date\": discharge_series.index.min(),\n",
    "                    \"end_date\": discharge_series.index.max(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            results[gauge_id] = metrics\n",
    "\n",
    "            if progress_bar:\n",
    "                iterator.set_postfix({\"status\": f\"completed - {len(metrics)} metrics\"})\n",
    "\n",
    "        except Exception as e:\n",
    "            failed_gauges.append((gauge_id, str(e)))\n",
    "            if progress_bar:\n",
    "                iterator.set_postfix({\"status\": f\"failed - {str(e)[:20]}...\"})\n",
    "            continue\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    if results:\n",
    "        results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "        results_df.index.name = \"gauge_id\"\n",
    "\n",
    "        print(f\"\\nSuccessfully processed {len(results_df)} gauge stations\")\n",
    "        print(f\"Failed to process {len(failed_gauges)} gauge stations\")\n",
    "\n",
    "        if failed_gauges:\n",
    "            print(\"Failed gauges:\")\n",
    "            for gauge_id, error in failed_gauges[:5]:  # Show first 5 failures\n",
    "                print(f\"  {gauge_id}: {error}\")\n",
    "            if len(failed_gauges) > 5:\n",
    "                print(f\"  ... and {len(failed_gauges) - 5} more\")\n",
    "\n",
    "        return results_df\n",
    "    else:\n",
    "        print(\"No gauge stations were successfully processed!\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e05a9d",
   "metadata": {},
   "source": [
    "## Use Case 1: Quick Analysis for All Stations\n",
    "\n",
    "Process all available gauge stations with a subset of key metrics for rapid assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick analysis for first 10 stations (for demonstration)\n",
    "sample_stations = discharge_data.columns[:10].tolist()\n",
    "\n",
    "print(\"=== USE CASE 1: Quick Analysis (Key Metrics Only) ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Quick analysis without BFI and detailed metrics for speed\n",
    "quick_results = calculate_hydrological_statistics_batch(\n",
    "    discharge_data,\n",
    "    gauge_ids=sample_stations,\n",
    "    min_data_years=1.0,\n",
    "    include_bfi=False,  # Skip BFI for speed\n",
    "    include_detailed_metrics=False,  # Only basic metrics\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nQuick analysis completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Generated {len(quick_results.columns)} metrics per station\")\n",
    "\n",
    "# Display first few results\n",
    "print(\"\\nSample results (first 5 stations, selected metrics):\")\n",
    "key_metrics = [\n",
    "    col\n",
    "    for col in quick_results.columns\n",
    "    if any(x in col.lower() for x in [\"mean\", \"cv\", \"q05\", \"q95\", \"fdc_slope\", \"data_years\"])\n",
    "]\n",
    "print(quick_results[key_metrics].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ff65a",
   "metadata": {},
   "source": [
    "## Use Case 2: Comprehensive Analysis for Selected Stations\n",
    "\n",
    "Perform detailed analysis including BFI calculation and all available metrics for a subset of high-quality stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1132383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select high-quality stations (those with >5 years of data)\n",
    "print(\"=== USE CASE 2: Comprehensive Analysis for High-Quality Stations ===\")\n",
    "\n",
    "# Find stations with sufficient data\n",
    "data_coverage = discharge_data.count() / len(discharge_data) * 100\n",
    "long_record_stations = data_coverage[data_coverage > 80].index[:5].tolist()  # Top 5 stations\n",
    "\n",
    "print(f\"Selected {len(long_record_stations)} high-quality stations for detailed analysis:\")\n",
    "for station in long_record_stations:\n",
    "    coverage = data_coverage[station]\n",
    "    print(f\"  {station}: {coverage:.1f}% data coverage\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Comprehensive analysis with all metrics including BFI\n",
    "comprehensive_results = calculate_hydrological_statistics_batch(\n",
    "    discharge_data,\n",
    "    gauge_ids=long_record_stations,\n",
    "    min_data_years=3.0,\n",
    "    include_bfi=True,  # Include BFI calculation\n",
    "    include_detailed_metrics=True,  # All available metrics\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nComprehensive analysis completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Generated {len(comprehensive_results.columns)} metrics per station\")\n",
    "\n",
    "# Display comprehensive results\n",
    "print(\"\\nComprehensive results overview:\")\n",
    "print(f\"Shape: {comprehensive_results.shape}\")\n",
    "print(f\"Metrics categories:\")\n",
    "\n",
    "# Group metrics by category\n",
    "metric_categories = {}\n",
    "for col in comprehensive_results.columns:\n",
    "    category = col.split(\"_\")[0] if \"_\" in col else \"basic\"\n",
    "    if category not in metric_categories:\n",
    "        metric_categories[category] = []\n",
    "    metric_categories[category].append(col)\n",
    "\n",
    "for category, metrics in metric_categories.items():\n",
    "    print(f\"  {category}: {len(metrics)} metrics\")\n",
    "\n",
    "comprehensive_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032e447",
   "metadata": {},
   "source": [
    "## Use Case 3: Custom Metrics for Specific Analysis\n",
    "\n",
    "Calculate specific sets of metrics for targeted research questions (e.g., drought analysis, flood analysis, baseflow studies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_drought_focused_metrics(\n",
    "    discharge_data: pd.DataFrame, gauge_ids: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Calculate metrics specifically focused on drought analysis.\"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for gauge_id in tqdm(gauge_ids, desc=\"Drought analysis\"):\n",
    "        try:\n",
    "            discharge_series = discharge_data[gauge_id].dropna()\n",
    "\n",
    "            if len(discharge_series) < 730:  # Skip if less than 2 years\n",
    "                continue\n",
    "\n",
    "            # Initialize analyzers\n",
    "            extremes = FlowExtremes(discharge_series)\n",
    "            variability = FlowVariability(discharge_series)\n",
    "            fdc = FlowDurationCurve(discharge_series)\n",
    "\n",
    "            # Drought-specific metrics\n",
    "            drought_metrics = extremes.calculate_drought_indices()\n",
    "            low_flow_metrics = extremes.analyze_low_flows(threshold_multiplier=0.1)  # Stricter threshold\n",
    "\n",
    "            # Additional drought indicators\n",
    "            q70 = fdc.get_percentile_flow(70)\n",
    "            q80 = fdc.get_percentile_flow(80)\n",
    "            q90 = fdc.get_percentile_flow(90)\n",
    "            q95 = fdc.get_percentile_flow(95)\n",
    "\n",
    "            # Combine drought-focused metrics\n",
    "            metrics = {\n",
    "                \"gauge_id\": gauge_id,\n",
    "                \"q70\": q70,\n",
    "                \"q80\": q80,\n",
    "                \"q90\": q90,\n",
    "                \"q95\": q95,\n",
    "                \"q95_mean_ratio\": drought_metrics[\"q95_flow\"] / np.mean(discharge_series),\n",
    "                \"low_flow_frequency\": low_flow_metrics[\"low_flow_frequency\"],\n",
    "                \"low_flow_duration_avg\": low_flow_metrics[\"low_flow_avg_duration\"],\n",
    "                \"low_flow_duration_max\": low_flow_metrics[\"low_flow_max_duration\"],\n",
    "                \"baseflow_approx\": drought_metrics[\"bfi_approx\"],\n",
    "                \"cv\": np.std(discharge_series) / np.mean(discharge_series),\n",
    "                \"data_years\": len(discharge_series) / 365.25,\n",
    "            }\n",
    "\n",
    "            results[gauge_id] = metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "\n",
    "\n",
    "def calculate_flood_focused_metrics(discharge_data: pd.DataFrame, gauge_ids: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Calculate metrics specifically focused on flood analysis.\"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for gauge_id in tqdm(gauge_ids, desc=\"Flood analysis\"):\n",
    "        try:\n",
    "            discharge_series = discharge_data[gauge_id].dropna()\n",
    "\n",
    "            if len(discharge_series) < 730:  # Skip if less than 2 years\n",
    "                continue\n",
    "\n",
    "            # Initialize analyzers\n",
    "            extremes = FlowExtremes(discharge_series)\n",
    "            variability = FlowVariability(discharge_series)\n",
    "            fdc = FlowDurationCurve(discharge_series)\n",
    "\n",
    "            # Flood-specific metrics\n",
    "            flood_metrics = extremes.calculate_flood_indices()\n",
    "            high_flow_metrics = extremes.analyze_high_flows(\n",
    "                threshold_multiplier=3.0\n",
    "            )  # Conservative threshold\n",
    "            flashiness = variability.calculate_flashiness_index()\n",
    "\n",
    "            # Additional flood indicators\n",
    "            q05 = fdc.get_percentile_flow(5)\n",
    "            q10 = fdc.get_percentile_flow(10)\n",
    "            q20 = fdc.get_percentile_flow(20)\n",
    "\n",
    "            # Combine flood-focused metrics\n",
    "            metrics = {\n",
    "                \"gauge_id\": gauge_id,\n",
    "                \"q05\": q05,\n",
    "                \"q10\": q10,\n",
    "                \"q20\": q20,\n",
    "                \"q05_mean_ratio\": q05 / np.mean(discharge_series),\n",
    "                \"high_flow_frequency\": high_flow_metrics[\"high_flow_frequency\"],\n",
    "                \"high_flow_duration_avg\": high_flow_metrics[\"high_flow_avg_duration\"],\n",
    "                \"high_flow_duration_max\": high_flow_metrics[\"high_flow_max_duration\"],\n",
    "                \"flashiness_index\": flashiness[\"flashiness_index\"],\n",
    "                \"max_daily_rise\": flood_metrics.get(\"max_daily_rise\", np.nan),\n",
    "                \"flood_threshold\": flood_metrics[\"flood_threshold\"],\n",
    "                \"data_years\": len(discharge_series) / 365.25,\n",
    "            }\n",
    "\n",
    "            results[gauge_id] = metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "\n",
    "\n",
    "# Example: Drought analysis for selected stations\n",
    "print(\"=== USE CASE 3A: Drought-Focused Analysis ===\")\n",
    "drought_results = calculate_drought_focused_metrics(discharge_data, sample_stations)\n",
    "print(f\"Drought analysis completed for {len(drought_results)} stations\")\n",
    "print(\"\\nDrought metrics summary:\")\n",
    "drought_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Flood analysis for selected stations\n",
    "print(\"=== USE CASE 3B: Flood-Focused Analysis ===\")\n",
    "flood_results = calculate_flood_focused_metrics(discharge_data, sample_stations)\n",
    "print(f\"Flood analysis completed for {len(flood_results)} stations\")\n",
    "print(\"\\nFlood metrics summary:\")\n",
    "flood_results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6459e03b",
   "metadata": {},
   "source": [
    "## Use Case 4: Flow Regime Classification\n",
    "\n",
    "Classify flow regimes across all stations using key hydrological signatures and create a comprehensive regime database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_flow_regimes(discharge_data: pd.DataFrame, gauge_ids: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Classify flow regimes using key hydrological signatures.\"\"\"\n",
    "\n",
    "    from src.hydro.flow_indices import calculate_regime_classification_metrics\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for gauge_id in tqdm(gauge_ids, desc=\"Regime classification\"):\n",
    "        try:\n",
    "            discharge_series = discharge_data[gauge_id].dropna()\n",
    "\n",
    "            if len(discharge_series) < 1095:  # Require at least 3 years\n",
    "                continue\n",
    "\n",
    "            # Calculate classification metrics\n",
    "            classification_metrics = calculate_regime_classification_metrics(discharge_series)\n",
    "\n",
    "            # Additional signature metrics\n",
    "            fdc = FlowDurationCurve(discharge_series)\n",
    "            variability = FlowVariability(discharge_series)\n",
    "\n",
    "            # Key signatures for classification\n",
    "            fdc_slope = fdc.calculate_fdc_slope()\n",
    "            cv = np.std(discharge_series) / np.mean(discharge_series)\n",
    "            q5_q95_ratio = fdc.get_percentile_flow(5) / fdc.get_percentile_flow(95)\n",
    "\n",
    "            # Seasonal signatures\n",
    "            if isinstance(discharge_series.index, pd.DatetimeIndex):\n",
    "                timing = FlowTiming(discharge_series)\n",
    "                seasonal_metrics = timing.calculate_seasonal_flows()\n",
    "\n",
    "                # Summer vs winter flow ratio\n",
    "                summer_ratio = seasonal_metrics.get(\"summer_ratio\", np.nan)\n",
    "                winter_ratio = seasonal_metrics.get(\"winter_ratio\", np.nan)\n",
    "            else:\n",
    "                summer_ratio = np.nan\n",
    "                winter_ratio = np.nan\n",
    "\n",
    "            # Enhanced regime classification\n",
    "            regime_type = classification_metrics[\"regime_classification\"]\n",
    "\n",
    "            # Additional classification based on seasonal patterns\n",
    "            if not np.isnan(summer_ratio) and not np.isnan(winter_ratio):\n",
    "                if summer_ratio > 1.5:\n",
    "                    regime_subtype = \"snowmelt_dominated\"\n",
    "                elif winter_ratio > 1.5:\n",
    "                    regime_subtype = \"rain_dominated\"\n",
    "                else:\n",
    "                    regime_subtype = \"mixed\"\n",
    "            else:\n",
    "                regime_subtype = \"unknown\"\n",
    "\n",
    "            metrics = {\n",
    "                \"gauge_id\": gauge_id,\n",
    "                \"regime_type\": regime_type,\n",
    "                \"regime_subtype\": regime_subtype,\n",
    "                \"fdc_slope\": fdc_slope,\n",
    "                \"coefficient_of_variation\": cv,\n",
    "                \"q5_q95_ratio\": q5_q95_ratio,\n",
    "                \"summer_ratio\": summer_ratio,\n",
    "                \"winter_ratio\": winter_ratio,\n",
    "                \"baseflow_ratio_approx\": classification_metrics[\"baseflow_ratio_approx\"],\n",
    "                \"mean_flow\": float(np.mean(discharge_series)),\n",
    "                \"data_years\": len(discharge_series) / 365.25,\n",
    "            }\n",
    "\n",
    "            results[gauge_id] = metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "\n",
    "\n",
    "print(\"=== USE CASE 4: Flow Regime Classification ===\")\n",
    "regime_results = classify_flow_regimes(discharge_data, sample_stations)\n",
    "print(f\"Regime classification completed for {len(regime_results)} stations\")\n",
    "\n",
    "# Display classification results\n",
    "print(\"\\nFlow regime distribution:\")\n",
    "print(regime_results[\"regime_type\"].value_counts())\n",
    "print(\"\\nRegime subtype distribution:\")\n",
    "print(regime_results[\"regime_subtype\"].value_counts())\n",
    "\n",
    "print(\"\\nRegime classification summary:\")\n",
    "regime_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb9099",
   "metadata": {},
   "source": [
    "## Use Case 5: Production Pipeline for Full Dataset\n",
    "\n",
    "Production-ready pipeline for processing the entire CAMELS-RU dataset with optimizations for large-scale processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c882b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_pipeline_full_dataset(\n",
    "    discharge_data: pd.DataFrame,\n",
    "    output_dir: str = \"../results/\",\n",
    "    chunk_size: int = 50,\n",
    "    save_intermediate: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Production pipeline for processing the full CAMELS-RU dataset.\n",
    "\n",
    "    Features:\n",
    "    - Chunked processing to manage memory\n",
    "    - Intermediate saves for fault tolerance\n",
    "    - Multiple output formats\n",
    "    - Quality control and validation\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Get all available gauge stations\n",
    "    all_stations = discharge_data.columns.tolist()\n",
    "    print(f\"Total stations to process: {len(all_stations)}\")\n",
    "\n",
    "    # Split into chunks for processing\n",
    "    chunks = [all_stations[i : i + chunk_size] for i in range(0, len(all_stations), chunk_size)]\n",
    "    print(f\"Processing in {len(chunks)} chunks of {chunk_size} stations each\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"\\n--- Processing Chunk {i + 1}/{len(chunks)} ---\")\n",
    "        print(f\"Stations {i * chunk_size + 1} to {min((i + 1) * chunk_size, len(all_stations))}\")\n",
    "\n",
    "        try:\n",
    "            # Process chunk with comprehensive metrics\n",
    "            chunk_results = calculate_hydrological_statistics_batch(\n",
    "                discharge_data,\n",
    "                gauge_ids=chunk,\n",
    "                min_data_years=1.0,\n",
    "                include_bfi=True,  # Include BFI for production\n",
    "                include_detailed_metrics=True,\n",
    "                progress_bar=True,\n",
    "            )\n",
    "\n",
    "            if len(chunk_results) > 0:\n",
    "                all_results.append(chunk_results)\n",
    "\n",
    "                # Save intermediate results\n",
    "                if save_intermediate:\n",
    "                    chunk_file = f\"{output_dir}/hydro_metrics_chunk_{i + 1:03d}.csv\"\n",
    "                    chunk_results.to_csv(chunk_file)\n",
    "                    print(f\"Saved intermediate results to {chunk_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i + 1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        final_results = pd.concat(all_results, axis=0)\n",
    "\n",
    "        # Quality control\n",
    "        print(f\"\\n--- Quality Control ---\")\n",
    "        print(f\"Total stations processed: {len(final_results)}\")\n",
    "        print(f\"Total metrics calculated: {len(final_results.columns)}\")\n",
    "\n",
    "        # Check for stations with insufficient data\n",
    "        insufficient_data = final_results[final_results[\"data_years\"] < 2.0]\n",
    "        if len(insufficient_data) > 0:\n",
    "            print(f\"Warning: {len(insufficient_data)} stations have less than 2 years of data\")\n",
    "\n",
    "        # Save final results in multiple formats\n",
    "        final_results.to_csv(f\"{output_dir}/camels_ru_hydrological_metrics_complete.csv\")\n",
    "        final_results.to_parquet(f\"{output_dir}/camels_ru_hydrological_metrics_complete.parquet\")\n",
    "\n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            \"processing_date\": pd.Timestamp.now().isoformat(),\n",
    "            \"total_stations\": len(final_results),\n",
    "            \"total_metrics\": len(final_results.columns),\n",
    "            \"data_period\": f\"{discharge_data.index.min()} to {discharge_data.index.max()}\",\n",
    "            \"metric_categories\": list(\n",
    "                set([col.split(\"_\")[0] for col in final_results.columns if \"_\" in col])\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        pd.Series(metadata).to_json(f\"{output_dir}/processing_metadata.json\", indent=2)\n",
    "\n",
    "        print(f\"\\nFinal results saved to {output_dir}\")\n",
    "        return final_results\n",
    "\n",
    "    else:\n",
    "        print(\"No stations were successfully processed!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# For demonstration, run on a subset (uncomment next lines to run full dataset)\n",
    "print(\"=== USE CASE 5: Production Pipeline (Demo with 20 stations) ===\")\n",
    "\n",
    "# Demo with first 20 stations\n",
    "demo_stations = discharge_data.columns[:20]\n",
    "demo_data = discharge_data[demo_stations]\n",
    "\n",
    "production_results = production_pipeline_full_dataset(\n",
    "    demo_data, output_dir=\"../results/demo/\", chunk_size=10, save_intermediate=True\n",
    ")\n",
    "\n",
    "print(f\"\\nProduction pipeline completed. Final dataset shape: {production_results.shape}\")\n",
    "\n",
    "# Generate summary statistics\n",
    "print(\"\\n--- Summary Statistics ---\")\n",
    "summary_stats = {\n",
    "    \"mean_data_years\": production_results[\"data_years\"].mean(),\n",
    "    \"stations_with_5plus_years\": (production_results[\"data_years\"] >= 5).sum(),\n",
    "    \"stations_with_10plus_years\": (production_results[\"data_years\"] >= 10).sum(),\n",
    "    \"mean_cv\": production_results[\"magnitude_ma16\"].mean()\n",
    "    if \"magnitude_ma16\" in production_results.columns\n",
    "    else np.nan,\n",
    "    \"stations_processed\": len(production_results),\n",
    "}\n",
    "\n",
    "for key, value in summary_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Display first few rows of final results\n",
    "production_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc5f4e3",
   "metadata": {},
   "source": [
    "## Data Export and Validation\n",
    "\n",
    "Export the calculated metrics in various formats and perform validation checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export functions for different use cases\n",
    "def export_key_metrics_summary(results_df: pd.DataFrame, output_path: str) -> None:\n",
    "    \"\"\"Export a summary table with only key hydrological metrics.\"\"\"\n",
    "\n",
    "    # Define key metrics for summary\n",
    "    key_metrics = [\n",
    "        \"data_years\",\n",
    "        \"data_completeness\",\n",
    "        \"magnitude_ma01\",  # Mean flow\n",
    "        \"magnitude_ma16\",  # CV\n",
    "        \"fdc_fdc_slope\",  # FDC slope\n",
    "        \"baseflow_bfi\",  # BFI\n",
    "        \"extreme_q05_flow\",\n",
    "        \"extreme_q95_flow\",  # Flow extremes\n",
    "        \"variability_flashiness_index\",  # Flashiness\n",
    "        \"timing_hfd_mean\",  # Half-flow date\n",
    "    ]\n",
    "\n",
    "    # Select available key metrics\n",
    "    available_metrics = [col for col in key_metrics if col in results_df.columns]\n",
    "    summary_df = results_df[available_metrics].copy()\n",
    "\n",
    "    # Add derived metrics\n",
    "    if \"extreme_q05_flow\" in summary_df.columns and \"extreme_q95_flow\" in summary_df.columns:\n",
    "        summary_df[\"flow_variability_ratio\"] = (\n",
    "            summary_df[\"extreme_q05_flow\"] / summary_df[\"extreme_q95_flow\"]\n",
    "        )\n",
    "\n",
    "    # Export with proper naming\n",
    "    summary_df.columns = [col.replace(\"_\", \" \").title() for col in summary_df.columns]\n",
    "    summary_df.to_csv(output_path)\n",
    "    print(f\"Key metrics summary exported to {output_path}\")\n",
    "\n",
    "\n",
    "def validate_results(results_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Perform validation checks on calculated metrics.\"\"\"\n",
    "\n",
    "    validation_report = {}\n",
    "\n",
    "    # Check for reasonable ranges\n",
    "    if \"baseflow_bfi\" in results_df.columns:\n",
    "        bfi_out_of_range = ((results_df[\"baseflow_bfi\"] < 0) | (results_df[\"baseflow_bfi\"] > 1)).sum()\n",
    "        validation_report[\"bfi_out_of_range\"] = bfi_out_of_range\n",
    "\n",
    "    # Check for extreme CV values\n",
    "    if \"magnitude_ma16\" in results_df.columns:\n",
    "        extreme_cv = (results_df[\"magnitude_ma16\"] > 5).sum()\n",
    "        validation_report[\"extreme_cv_count\"] = extreme_cv\n",
    "\n",
    "    # Check data completeness\n",
    "    missing_data_severe = (results_df[\"data_completeness\"] < 50).sum()\n",
    "    validation_report[\"severe_missing_data_count\"] = missing_data_severe\n",
    "\n",
    "    # Check for NaN values in key metrics\n",
    "    key_metrics_nan = {}\n",
    "    for col in [\"magnitude_ma01\", \"fdc_fdc_slope\", \"baseflow_bfi\"]:\n",
    "        if col in results_df.columns:\n",
    "            key_metrics_nan[col] = results_df[col].isna().sum()\n",
    "\n",
    "    validation_report[\"nan_counts\"] = key_metrics_nan\n",
    "    validation_report[\"total_stations\"] = len(results_df)\n",
    "\n",
    "    return validation_report\n",
    "\n",
    "\n",
    "# Perform exports and validation on our comprehensive results\n",
    "if \"comprehensive_results\" in locals() and len(comprehensive_results) > 0:\n",
    "    print(\"=== EXPORT AND VALIDATION ===\")\n",
    "\n",
    "    # Export key metrics summary\n",
    "    export_key_metrics_summary(comprehensive_results, \"../results/key_metrics_summary.csv\")\n",
    "\n",
    "    # Perform validation\n",
    "    validation_report = validate_results(comprehensive_results)\n",
    "\n",
    "    print(\"\\nValidation Report:\")\n",
    "    for key, value in validation_report.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Create metrics catalog\n",
    "    metrics_catalog = pd.DataFrame(\n",
    "        {\n",
    "            \"metric_name\": comprehensive_results.columns,\n",
    "            \"category\": [\n",
    "                col.split(\"_\")[0] if \"_\" in col else \"basic\" for col in comprehensive_results.columns\n",
    "            ],\n",
    "            \"non_null_count\": [\n",
    "                comprehensive_results[col].notna().sum() for col in comprehensive_results.columns\n",
    "            ],\n",
    "            \"mean_value\": [\n",
    "                comprehensive_results[col].mean()\n",
    "                if pd.api.types.is_numeric_dtype(comprehensive_results[col])\n",
    "                else np.nan\n",
    "                for col in comprehensive_results.columns\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    metrics_catalog.to_csv(\"../results/metrics_catalog.csv\", index=False)\n",
    "    print(\"\\nMetrics catalog exported to ../results/metrics_catalog.csv\")\n",
    "\n",
    "    # Display metrics by category\n",
    "    print(\"\\nMetrics by category:\")\n",
    "    print(metrics_catalog.groupby(\"category\").size().sort_values(ascending=False))\n",
    "\n",
    "else:\n",
    "    print(\"No comprehensive results available for export. Run the comprehensive analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6cf20f",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook demonstrates comprehensive usage of the reorganized hydrological metrics package. The modular design allows for:\n",
    "\n",
    "### ✅ What We've Accomplished:\n",
    "1. **Batch Processing**: Efficient calculation of metrics for multiple stations\n",
    "2. **Flexible Analysis**: Different use cases from quick analysis to comprehensive metrics\n",
    "3. **Specialized Metrics**: Drought, flood, and regime-specific analysis\n",
    "4. **Production Pipeline**: Scalable processing for large datasets\n",
    "5. **Quality Control**: Validation and export capabilities\n",
    "\n",
    "### 🎯 Key Benefits:\n",
    "- **Standardized Output**: All functions return DataFrames with gauge_id as index\n",
    "- **Error Handling**: Robust processing that continues despite individual station failures\n",
    "- **Performance**: Optimized calculations with progress tracking\n",
    "- **Flexibility**: Easy to customize for specific research questions\n",
    "- **Scalability**: Chunked processing for large datasets\n",
    "\n",
    "### 📊 Typical Output Structure:\n",
    "```\n",
    "DataFrame with gauge_id as index and columns like:\n",
    "- magnitude_* : Flow magnitude metrics (mean, median, CV, etc.)\n",
    "- fdc_* : Flow Duration Curve metrics (slope, percentiles, etc.)\n",
    "- extreme_* : Extreme flow metrics (floods, droughts, quantiles)\n",
    "- timing_* : Temporal metrics (seasonal patterns, timing of extremes)\n",
    "- variability_* : Variability metrics (flashiness, autocorrelation, etc.)\n",
    "- baseflow_* : Base flow metrics (BFI, baseflow statistics)\n",
    "- data_* : Data quality metrics (years, completeness, etc.)\n",
    "```\n",
    "\n",
    "### 🚀 Recommended Workflow:\n",
    "1. **Quick Analysis** → Get overview of all stations\n",
    "2. **Quality Filtering** → Select high-quality stations\n",
    "3. **Comprehensive Analysis** → Calculate detailed metrics for selected stations\n",
    "4. **Specialized Analysis** → Focus on specific phenomena (droughts, floods)\n",
    "5. **Export & Archive** → Save results in multiple formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b888b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: One-liner for comprehensive analysis\n",
    "print(\"=== ONE-LINER COMPREHENSIVE ANALYSIS ===\")\n",
    "print(\"For users who want everything calculated with minimal code:\")\n",
    "print()\n",
    "print(\"# Single function call to get comprehensive metrics for all stations:\")\n",
    "print(\"comprehensive_metrics = calculate_hydrological_statistics_batch(\")\n",
    "print(\"    discharge_data=discharge_data,\")\n",
    "print(\"    min_data_years=2.0,\")\n",
    "print(\"    include_bfi=True,\")\n",
    "print(\"    include_detailed_metrics=True\")\n",
    "print(\")\")\n",
    "print()\n",
    "print(\"# Result: DataFrame with gauge_id as index and ~100+ hydrological metrics as columns\")\n",
    "print(\"# Ready for further analysis, machine learning, or export\")\n",
    "\n",
    "# Example of what the final DataFrame looks like\n",
    "if \"comprehensive_results\" in locals() and len(comprehensive_results) > 0:\n",
    "    print(f\"\\nExample output shape: {comprehensive_results.shape}\")\n",
    "    print(\"Sample metrics preview:\")\n",
    "    sample_cols = [\n",
    "        col\n",
    "        for col in comprehensive_results.columns\n",
    "        if any(x in col for x in [\"ma01\", \"bfi\", \"fdc_slope\", \"q05\", \"q95\"])\n",
    "    ][:5]\n",
    "    if sample_cols:\n",
    "        print(comprehensive_results[sample_cols].head(3).round(3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK COMPLETE - Ready for CAMELS-RU Production Analysis!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
